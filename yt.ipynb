{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea9c8495",
   "metadata": {},
   "source": [
    "pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c58c480",
   "metadata": {},
   "source": [
    "pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c306c0b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UUltdp3Q-Bnf5ZAgEDqfErAw\n",
      "166\n",
      "        video_id      channelTitle  \\\n",
      "0    -ltkYl9gpCI  Gangster Records   \n",
      "1    fgHQOvW6CUs  Gangster Records   \n",
      "2    E598mcPwiig  Gangster Records   \n",
      "3    8ekX8EVFqXY  Gangster Records   \n",
      "4    S3h1ZDFKaic  Gangster Records   \n",
      "..           ...               ...   \n",
      "161  b2yMbPbo5k4  Gangster Records   \n",
      "162  0HfHaRc-n3s  Gangster Records   \n",
      "163  XIXxvr9lHOM  Gangster Records   \n",
      "164  j3mNGjr4LMY  Gangster Records   \n",
      "165  oNhrCXRrck8  Gangster Records   \n",
      "\n",
      "                                                 title  \\\n",
      "0    Edward Maya & Vika Jigulina - Stereo Love (JAR...   \n",
      "1        Brennan Savage - Look At Me Now (FRHAD remix)   \n",
      "2                     Whoopty (GANGSTER RECORDS remix)   \n",
      "3    Chris Brown - Under The Influence (RIMINIRS re...   \n",
      "4    Post Malone Rockstar ft. 21 Savage (GANGSTER R...   \n",
      "..                                                 ...   \n",
      "161                           Mezdeke - (Beledi Remix)   \n",
      "162  Lil Nas X, Jack Harlow - INDUSTRY BABY (CaZzin...   \n",
      "163  CKay - Love Nwantiti (NORTKASH x TheBlvcks REMIX)   \n",
      "164               Inta Eyh - XZEEZ Remix (Nancy Ajram)   \n",
      "165             CJ - Whoopty (Robert Cristian Remix) ♛   \n",
      "\n",
      "                                           description  \\\n",
      "0    #carmusic #applemusic #gangsterrecords #tiktok...   \n",
      "1    #carmusic #applemusic #gangsterrecords #tiktok...   \n",
      "2    #carmusic #whoopty2024 #applemusic #gangsterre...   \n",
      "3    #carmusic #applemusic #gangsterrecords #tiktok...   \n",
      "4    #bassboosted #applemusic #Gangsterrecords \\n\\n...   \n",
      "..                                                 ...   \n",
      "161  © No copyright infringements intended. If you ...   \n",
      "162  © No copyright infringements intended. If you ...   \n",
      "163  © No copyright infringements intended. If you ...   \n",
      "164  © No copyright infringements intended. If you ...   \n",
      "165  © No copyright infringements intended. If you ...   \n",
      "\n",
      "                                                  tags           publishedAt  \\\n",
      "0    [Stereo Love, Stereo Love sped up, Stereo Love...  2023-11-26T19:28:16Z   \n",
      "1                                                 None  2023-11-25T19:36:49Z   \n",
      "2    [Car music, Car song, Car drifting, Appel musi...  2023-11-24T20:25:23Z   \n",
      "3    [Chris Brown - Under The Influence (Riminirs r...  2023-11-24T17:05:03Z   \n",
      "4                                                 None  2023-08-18T21:33:39Z   \n",
      "..                                                 ...                   ...   \n",
      "161  [car music, mix 2021, car music mix, best gang...  2021-12-16T06:23:21Z   \n",
      "162  [lil nas x, jack harlow, lil nas x jack harlow...  2021-12-15T19:40:33Z   \n",
      "163  [CKay - Love Nwantiti (NORTKASH x TheBlvcks |R...  2021-12-15T18:41:36Z   \n",
      "164  [Gangster Records, music, carmusic, deep house...  2021-12-15T13:15:35Z   \n",
      "165  [2020, ALok, Alex Lists, Alok, Arash, Bass Bos...  2021-12-15T12:20:29Z   \n",
      "\n",
      "    viewCount likeCount favouriteCount commentCount duration definition  \\\n",
      "0        2052        28           None            3  PT2M54S         hd   \n",
      "1         776        13           None            0  PT3M31S         hd   \n",
      "2         936        11           None            1  PT2M57S         hd   \n",
      "3        6560        75           None            3  PT2M57S         hd   \n",
      "4        6598        79           None            2   PT3M7S         hd   \n",
      "..        ...       ...            ...          ...      ...        ...   \n",
      "161      2961      None           None            0  PT4M46S         hd   \n",
      "162        76      None           None            2  PT2M39S         hd   \n",
      "163        66      None           None            1  PT2M19S         hd   \n",
      "164        71      None           None            0  PT4M40S         hd   \n",
      "165       156      None           None            1   PT4M4S         hd   \n",
      "\n",
      "    caption  \n",
      "0     false  \n",
      "1     false  \n",
      "2     false  \n",
      "3     false  \n",
      "4     false  \n",
      "..      ...  \n",
      "161   false  \n",
      "162   false  \n",
      "163   false  \n",
      "164   false  \n",
      "165   false  \n",
      "\n",
      "[166 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import PIL\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "\n",
    "#channelid = st.text_input(\"Enter the channelid\")\n",
    "channelid = \"UCltdp3Q-Bnf5ZAgEDqfErAw\"\n",
    "# Set up the API client\n",
    "api_key = \"AIzaSyAjIU6fTQfKcnSsUkJ3-W-dQS8JBAGAmrU\"\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Make a request to retrieve video details\n",
    "request = youtube.channels().list(\n",
    "    part='snippet,ContentDetails,statistics',\n",
    "    maxResults = 50,\n",
    "    id=channelid\n",
    ")\n",
    "response1 = request.execute()\n",
    "#print(response)\n",
    "\n",
    "#For channel information\n",
    "all_data1 = []\n",
    "playlist = ''\n",
    "#print(response)\n",
    "\n",
    "for item in response1[\"items\"]:\n",
    "    data = {\n",
    "        'channelname':item['snippet']['title'],\n",
    "        'channel_id':item['id'],\n",
    "        'sub_count':item['statistics']['subscriberCount'],\n",
    "        'Total_Views': item['statistics']['viewCount'],\n",
    "        'description':item['snippet']['description'],\n",
    "        'playlist_id': item['contentDetails']['relatedPlaylists']['uploads'],\n",
    "        'place':item['snippet']['country'],\n",
    "        'published':item['snippet']['publishedAt'],\n",
    "        'Total_Videos':item['statistics']['videoCount'],\n",
    "    }\n",
    "\n",
    "    playlist = data['playlist_id']\n",
    "    all_data1.append(data)\n",
    "df = pd.DataFrame(all_data1)\n",
    "st.write(df)\n",
    "\n",
    "playlistid = playlist\n",
    "print(playlistid)\n",
    "# Make a request to retrieve video details\n",
    "request = youtube.playlistItems().list(\n",
    "    part='snippet,ContentDetails',\n",
    "    playlistId=playlistid,\n",
    "    maxResults = 50\n",
    ")\n",
    "response2 = request.execute()\n",
    "#print(response2)\n",
    "\n",
    "#GETTING VIDEOID's\n",
    "video_ids = []\n",
    "for item in response2['items']:\n",
    "    video_ids.append(item[\"contentDetails\"][\"videoId\"])\n",
    "next_page_token = response2.get(\"nextPageToken\")\n",
    "while next_page_token is not None:\n",
    "    request = youtube.playlistItems().list(\n",
    "        part = \"snippet,contentDetails\",\n",
    "        playlistId = playlistid,\n",
    "        maxResults = 50,\n",
    "        pageToken = next_page_token\n",
    "    )\n",
    "    response2 = request.execute()\n",
    "    \n",
    "    for item in response2['items']:\n",
    "        video_ids.append(item[\"contentDetails\"][\"videoId\"])\n",
    "    next_page_token = response2.get(\"nextPageToken\")\n",
    "    \n",
    "all_video_info = []\n",
    "print(len(video_ids))\n",
    "\n",
    "for i in range(0, len(video_ids), 50):\n",
    "    request = youtube.videos().list(\n",
    "        part = 'snippet,contentDetails,statistics',\n",
    "        id = ','.join(video_ids[i:i+50])\n",
    "    )\n",
    "    response3 = request.execute()\n",
    "    #print(response3)\n",
    "    \n",
    "    for video in response3['items']:\n",
    "        stats_to_keep = {\n",
    "            'snippet':['channelTitle', 'title', 'description', 'tags', 'publishedAt'],\n",
    "            'statistics':['viewCount', 'likeCount', 'favouriteCount', 'commentCount'],\n",
    "            'contentDetails':['duration', 'definition', 'caption']\n",
    "        }\n",
    "        video_info = {}\n",
    "        video_info['video_id'] = video['id']\n",
    "        \n",
    "        for k in stats_to_keep.keys():\n",
    "            for v in stats_to_keep[k]:\n",
    "                try:\n",
    "                    video_info[v] = video[k][v]\n",
    "                except:\n",
    "                    video_info[v] = None\n",
    "                    \n",
    "        all_video_info.append(video_info)\n",
    "        \n",
    "df1 = pd.DataFrame(all_video_info)\n",
    "print(df1)\n",
    "st.write(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e1d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f26871",
   "metadata": {},
   "source": [
    "pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66823dcd",
   "metadata": {},
   "source": [
    "pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12b4a4d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel Name: Music Everyday\n",
      "Total Subscribers: 184000\n",
      "Total Views: 3787379\n",
      "{'likes': '', 'uploads': 'UUk35YdCoXoFSEEiU-MUeMAQ'}\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import PIL\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "\n",
    "#channelid = st.text_input(\"Enter the channelid\")\n",
    "channelid = \"UCk35YdCoXoFSEEiU-MUeMAQ\"\n",
    "# Set up the API client\n",
    "api_key = \"AIzaSyAjIU6fTQfKcnSsUkJ3-W-dQS8JBAGAmrU\"\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Make a request to retrieve video details\n",
    "request = youtube.channels().list(\n",
    "    part='snippet,ContentDetails,statistics',\n",
    "    maxResults = 50,\n",
    "    id=channelid\n",
    ")\n",
    "response = request.execute()\n",
    "channel_title = response['items'][0]['snippet']['title']\n",
    "print(f'Channel Name: {channel_title}')\n",
    "total_subscribers = response['items'][0]['statistics']['subscriberCount']\n",
    "print(f'Total Subscribers: {total_subscribers}')\n",
    "total_views = response['items'][0]['statistics']['viewCount']\n",
    "print(f'Total Views: {total_views}')\n",
    "print(response['items'][0]['contentDetails']['relatedPlaylists'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269fba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_playlist = youtube.playlists().list(\n",
    "    part='snippet,contentDetails',\n",
    "    maxResults = 50,\n",
    "    id=channelid\n",
    ")\n",
    "response1 = request_playlist.execute()\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9288daf9",
   "metadata": {},
   "source": [
    "# Making Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d95d1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING MODULES\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import streamlit as st\n",
    "\n",
    "def make_connection():\n",
    "    api_key = 'AIzaSyAjIU6fTQfKcnSsUkJ3-W-dQS8JBAGAmrU'\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    return youtube\n",
    "\n",
    "\n",
    "youtube = make_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7edda622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#channel_id = 'UCpNUYWW0kiqyh0j5Qy3aU7w' #misra turp\n",
    "channel_id = 'UCV8e2g4IWQqK71bbzGDEI4Q' #Data Professor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81833ca",
   "metadata": {},
   "source": [
    "# Getting Channel information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "575bf4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_channel_info(channel_id):\n",
    "    request = youtube.channels().list(\n",
    "        part = 'snippet, ContentDetails, Statistics', \n",
    "        id = channel_id, \n",
    "        maxResults = 50\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    for i in range(0, len(response['items'])):\n",
    "        data = {\n",
    "            'channel_name' : response['items'][i]['snippet']['title'], \n",
    "            'channel_id' : response['items'][i]['id'], \n",
    "            'subscription_count' : response['items'][i]['statistics']['subscriberCount'], \n",
    "            'views' : response['items'][i]['statistics']['viewCount'], \n",
    "            'videos_count' : response['items'][i]['statistics']['videoCount'], \n",
    "            'channel_description' : response['items'][i]['snippet']['description'], \n",
    "            'playlist_id' : response['items'][i]['contentDetails']['relatedPlaylists']['uploads']\n",
    "        }\n",
    "    \n",
    "    return data\n",
    "\n",
    "#retrieve_channel_info(channel_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79667bc1",
   "metadata": {},
   "source": [
    "# Getting Playlist Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5206b709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def retrieve_playlist_info(channel_id):\n",
    "    playlist_data = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.playlists().list(\n",
    "                part='snippet, contentDetails',\n",
    "                channelId=channel_id,\n",
    "                pageToken=next_page_token,\n",
    "                maxResults=50,\n",
    "            )\n",
    "            response = request.execute()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error making API request: {e}\")\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            data = {\n",
    "                'playlist_id': item['id'],\n",
    "                'title': item['snippet']['title'],\n",
    "                'channel_id': item['snippet']['channelId'],\n",
    "                'channel_name': item['snippet']['channelTitle'],\n",
    "                'publication_timestamp': item['snippet']['publishedAt'],\n",
    "                'video_count': item['contentDetails']['itemCount']\n",
    "            }\n",
    "            playlist_data.append(data)\n",
    "        \n",
    "        next_page_token = response.get('nextPageToken')\n",
    "\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return playlist_data\n",
    "\n",
    "\n",
    "#retrieve_playlist_info(channel_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba24117e",
   "metadata": {},
   "source": [
    "# Getting Channel Videos Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bbfbd7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def retrieve_video_ids(channel_id):\n",
    "    retrieve_playlistid = retrieve_channel_info(channel_id)\n",
    "    \n",
    "    playlist_id =  retrieve_playlistid['playlist_id']\n",
    "        \n",
    "    next_page_token = None\n",
    "    video_ids = []\n",
    "    \n",
    "    while True:\n",
    "        request = youtube.playlistItems().list(\n",
    "            part = 'snippet', \n",
    "            playlistId = playlist_id, \n",
    "            pageToken = next_page_token, \n",
    "            maxResults = 50\n",
    "        )\n",
    "        \n",
    "        response = request.execute()\n",
    "        \n",
    "        for i in range(len(response['items'])):\n",
    "            video_ids.append(response['items'][i]['snippet']['resourceId']['videoId'])\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        \n",
    "        if not next_page_token:\n",
    "            break\n",
    "            \n",
    "    return video_ids\n",
    "\n",
    "\n",
    "#len(retrieve_video_ids(channel_id))\n",
    "# video_ids = retrieve_video_ids(channel_id)\n",
    "# print(video_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7679b028",
   "metadata": {},
   "source": [
    "# Getting Channel Video Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45a59696",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def retrieve_video_info(video_ids):\n",
    "    video_data = []\n",
    "    \n",
    "    for video_id in video_ids:\n",
    "\n",
    "        request = youtube.videos().list(\n",
    "            part = 'snippet, contentDetails, statistics', \n",
    "            id = video_id\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response['items']:\n",
    "            data = {\n",
    "                'channel_name' : item['snippet']['channelTitle'], \n",
    "                'channel_id' : item['snippet']['channelId'], \n",
    "                'video_id' : item['id'], \n",
    "                'title' : item['snippet']['title'], \n",
    "                'tags' : item['snippet'].get('tags', []), \n",
    "                'thumbnail' : item['snippet']['thumbnails']['default']['url'], \n",
    "                'description' : item['snippet']['description'], \n",
    "                'publication_timestamp' : item['snippet']['publishedAt'], \n",
    "                'duration' : item['contentDetails']['duration'], \n",
    "                'views' : item['statistics']['viewCount'], \n",
    "                'likes' : item['statistics'].get('likeCount'), \n",
    "                'comments' : item['statistics'].get('commentCount'), \n",
    "                'favorite_count' : item['statistics']['favoriteCount'], \n",
    "                'definition' : item['contentDetails']['definition'], \n",
    "                'caption_status' : item['contentDetails']['caption']\n",
    "            }\n",
    "            video_data.append(data)\n",
    "\n",
    "\n",
    "    return video_data\n",
    "                \n",
    "    \n",
    "#video_ids = retrieve_video_ids(channel_id)\n",
    "#print(video_ids)\n",
    "#len(retrieve_video_info(video_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3cb1fd",
   "metadata": {},
   "source": [
    "# Getting Comment Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "41896ff1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def retrieve_comment_info(video_ids):\n",
    "    comment_data = []\n",
    "    \n",
    "    try:\n",
    "        for video_id in video_ids:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part = 'snippet', \n",
    "                videoId = video_id, \n",
    "                maxResults = 50\n",
    "            )\n",
    "            response = request.execute()\n",
    "            \n",
    "            for item in response['items']:\n",
    "                data = {\n",
    "                    'comment_id' : item['snippet']['topLevelComment']['id'], \n",
    "                    'video_id' : item['snippet']['videoId'], \n",
    "                    'comment_text' : item['snippet']['topLevelComment']['snippet']['textOriginal'], \n",
    "                    'comment_author' : item['snippet']['topLevelComment']['snippet']['authorDisplayName'], \n",
    "                    'comment_timestamp' : item['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "                }\n",
    "                \n",
    "                comment_data.append(data)\n",
    "                \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return comment_data\n",
    "            \n",
    "\n",
    "#retrieve_comment_info(video_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1e8c83",
   "metadata": {},
   "source": [
    "# Connecting to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00aacd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB:)\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient \n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "\n",
    "uri = 'mongodb+srv://wolfx:wolfxkills@cluster0.hszoe9p.mongodb.net/?retryWrites=true&w=majority'\n",
    "    \n",
    "client = MongoClient(uri, server_api = ServerApi('1'), tz_aware = False, connect = True)\n",
    "\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print('Pinged your deployment. You successfully connected to MongoDB:)')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'Connection failed due to {e}')\n",
    "    \n",
    "db = client['Youtube_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6429716",
   "metadata": {},
   "source": [
    "# Upload to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af87adee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'upload completed successfully'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def channel_details(channel_id):\n",
    "    chl_details = retrieve_channel_info(channel_id)\n",
    "    plylst_details = retrieve_playlist_info(channel_id)\n",
    "    vdo_ids = retrieve_video_ids(channel_id)\n",
    "    vdo_details = retrieve_video_info(vdo_ids)\n",
    "    comnt_details = retrieve_comment_info(vdo_ids)\n",
    "    \n",
    "    coll1 = db['channel_details']\n",
    "    coll1.insert_one({\n",
    "        'channel_information' : chl_details, 'playlist_information' : plylst_details, 'video_information' : vdo_details, \n",
    "        'comment_information' : comnt_details\n",
    "    })\n",
    "    \n",
    "    return 'upload completed successfully'\n",
    "    \n",
    "channel_details(channel_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0759a9d5",
   "metadata": {},
   "source": [
    "pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26382e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "    host = 'localhost', \n",
    "    user = 'root', \n",
    "    password = 'Abu#@7899#', \n",
    "    database = 'youtube_data', \n",
    "    port = '3306'\n",
    ")\n",
    "\n",
    "cursor = mydb.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78773ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_formatter(published_time_yt):\n",
    "    parsed_timestamp = datetime.strptime(published_time_yt, '%Y-%m-%dT%H:%M:%SZ')\n",
    "    formatted_timestamp = parsed_timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return formatted_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09856d22",
   "metadata": {},
   "source": [
    "DURATION DESIGNATOR(FOR A PERIOD) INCLUDES YEAR, MONTH, WEEK, DAY AND \n",
    "TIME DESIGNATOR INCLUDES HOUR, MINUTES, SECONDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9468822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration_formatter(duration):\n",
    "    duration = duration[2:]\n",
    "    parsed_duration = timedelta()\n",
    "    time_part = ''\n",
    "    \n",
    "    for char in duration:\n",
    "        if char.isdigit():\n",
    "            time_part += char\n",
    "            \n",
    "        else:\n",
    "            if char == 'H':\n",
    "                parsed_duration += timedelta(hours = int(time_part))\n",
    "            elif char == 'M':\n",
    "                parsed_duration += timedelta(minutes = int(time_part))\n",
    "            elif char == 'S':\n",
    "                parsed_duration += timedelta(seconds = int(time_part))\n",
    "            time_part = ''\n",
    "            \n",
    "    return parsed_duration\n",
    "    \n",
    "#print(duration_formatter('PT3M22S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26091574",
   "metadata": {},
   "source": [
    "# Channels_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63e2d2d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def channels_table(cursor):\n",
    "    drop_query = 'DROP TABLE IF EXISTS channels'\n",
    "    cursor.execute(drop_query)\n",
    "    mydb.commit()\n",
    "\n",
    "    try:\n",
    "        create_query = '''\n",
    "        CREATE TABLE IF NOT EXISTS channels(\n",
    "        channel_name VARCHAR(100), \n",
    "        channel_id VARCHAR(80) PRIMARY KEY, \n",
    "        subscription_count BIGINT, \n",
    "        views BIGINT, \n",
    "        videos_count INT, \n",
    "        channel_description TEXT, \n",
    "        playlist_id VARCHAR(50)\n",
    "        )\n",
    "        '''\n",
    "\n",
    "        cursor.execute(create_query)\n",
    "        mydb.commit()\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        if err.errno == 1062:\n",
    "            print('channels tables already created')\n",
    "\n",
    "        else:\n",
    "            print(f'Error: {err}')\n",
    "\n",
    "    ch_list = []\n",
    "    db = client['Youtube_data']\n",
    "    coll1 = db['channel_details']\n",
    "\n",
    "    for ch_data in coll1.find({}, {'_id': 0, 'channel_information': 1}):\n",
    "        ch_list.append(ch_data['channel_information'])\n",
    "        \n",
    "    df = pd.DataFrame(ch_list)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        insert_query = '''\n",
    "        INSERT INTO channels(\n",
    "        channel_name, \n",
    "        channel_id, \n",
    "        subscription_count, \n",
    "        views, \n",
    "        videos_count, \n",
    "        channel_description, \n",
    "        playlist_id\n",
    "        )\n",
    "        VALUES(%s, %s, %s, %s, %s, %s, %s)\n",
    "        '''\n",
    "        \n",
    "        values = (\n",
    "            row['channel_name'], \n",
    "            row['channel_id'], \n",
    "            row['subscription_count'], \n",
    "            row['views'], \n",
    "            row['videos_count'], \n",
    "            row['channel_description'], \n",
    "            row['playlist_id']\n",
    "        )\n",
    "        \n",
    "        try: \n",
    "            cursor.execute(insert_query, values)\n",
    "            mydb.commit()\n",
    "            \n",
    "        except mysql.connector.Error as err:\n",
    "            if err.errno == 1062:  # MySQL duplicate entry code\n",
    "                print('Channels values already inserted')\n",
    "            \n",
    "            else :\n",
    "                print(f'Error: {err}')\n",
    "    \n",
    "    \n",
    "#channels_table(cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0465809",
   "metadata": {},
   "source": [
    "# playlists_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff5903c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def playlists_table(cursor):\n",
    "    drop_query = 'DROP TABLE IF EXISTS playlists'\n",
    "    cursor.execute(drop_query)\n",
    "    mydb.commit()\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        create_query = '''\n",
    "        CREATE TABLE IF NOT EXISTS playlists(\n",
    "        playlist_id VARCHAR(100) PRIMARY KEY, \n",
    "        title VARCHAR(80), \n",
    "        channelid VARCHAR(100), \n",
    "        channelname VARCHAR(100), \n",
    "        publication_timestamp TIMESTAMP, \n",
    "        video_count INT\n",
    "        )\n",
    "        '''\n",
    "        cursor.execute(create_query)\n",
    "        mydb.commit()\n",
    "        \n",
    "    except mysql.connector.Error as err:\n",
    "        if err.errno == 1062:\n",
    "            print('Playlists values already inserted')\n",
    "            \n",
    "        else :\n",
    "            print(f'Error: {err}')\n",
    "        \n",
    "    db = client['Youtube_data']\n",
    "    coll1 = db['channel_details']\n",
    "    ply_lists = []\n",
    "    for ply_list in coll1.find({}, {'_id': 0, 'playlist_information': 1}):\n",
    "        ply_list_data = ply_list['playlist_information']\n",
    "        for i in range(len(ply_list_data)):\n",
    "            ply_lists.append(ply_list_data[i])\n",
    "            \n",
    "    df = pd.DataFrame(ply_lists)\n",
    "    for index, row in df.iterrows():\n",
    "        publication_timestamp = date_formatter(row['publication_timestamp'])\n",
    "        \n",
    "        insert_query = '''\n",
    "        INSERT INTO playlists(\n",
    "        playlist_id, \n",
    "        title, \n",
    "        channelid, \n",
    "        channelname, \n",
    "        publication_timestamp, \n",
    "        video_count\n",
    "        )\n",
    "        VALUES(%s, %s, %s, %s, %s, %s)\n",
    "        '''\n",
    "        \n",
    "        values = (\n",
    "            row['playlist_id'], \n",
    "            row['title'], \n",
    "            row['channel_id'], \n",
    "            row['channel_name'], \n",
    "            publication_timestamp, \n",
    "            row['video_count']\n",
    "        )\n",
    "        try:\n",
    "            cursor.execute(insert_query, values)\n",
    "            mydb.commit()\n",
    "        \n",
    "        except mysql.connector.Error as err:\n",
    "            if err.errno == 1062:\n",
    "                print('playlists values already inserted')\n",
    "            else:\n",
    "                print(f'Error: {err}')\n",
    "    \n",
    "    \n",
    "#playlists_table(cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e8d2bb",
   "metadata": {},
   "source": [
    "# Videos tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93c7b093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def videos_table(cursor):\n",
    "    drop_query = 'DROP TABLE IF EXISTS videos'\n",
    "    cursor.execute(drop_query)\n",
    "    mydb.commit()\n",
    "    \n",
    "    try:\n",
    "        create_query = '''\n",
    "        CREATE TABLE IF NOT EXISTS videos(\n",
    "        channel_name VARCHAR(100), \n",
    "        channel_id VARCHAR(80), \n",
    "        video_id VARCHAR(80) PRIMARY KEY, \n",
    "        title VARCHAR(150), \n",
    "        tags TEXT, \n",
    "        thumbnail VARCHAR(225), \n",
    "        description TEXT, \n",
    "        publication_timestamp TIMESTAMP, \n",
    "        duration TIME, \n",
    "        views BIGINT, \n",
    "        likes BIGINT, \n",
    "        comments INT, \n",
    "        favorite_count INT, \n",
    "        definition VARCHAR(20), \n",
    "        caption_status VARCHAR(50)\n",
    "        )\n",
    "        '''\n",
    "        \n",
    "        cursor.execute(create_query)\n",
    "        mydb.commit()\n",
    "        \n",
    "    except mysql.connector.Error as err:\n",
    "        if err.errno == 1062:\n",
    "            print('Videos table already created')\n",
    "            \n",
    "        else:\n",
    "            print(f'Error: {err}')\n",
    "            \n",
    "    db  = client['Youtube_data']\n",
    "    coll1 = db['channel_details']\n",
    "    vdo_lists = []\n",
    "    for vdo_list in coll1.find({}, {'_id': 0, 'video_information': 1}):\n",
    "        vdo_list_data = vdo_list['video_information']\n",
    "        for _ in range(len(vdo_list_data)):\n",
    "            vdo_lists.append(vdo_list_data[_])\n",
    "    \n",
    "    df = pd.DataFrame(vdo_lists)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        publication_timestamp = date_formatter(row['publication_timestamp'])\n",
    "        duration = duration_formatter(row['duration'])\n",
    "        tags = ', '.join(map(str, row['tags'])) if row['tags'] else None\n",
    "        \n",
    "        insert_query = '''\n",
    "        INSERT INTO videos(\n",
    "        channel_name, \n",
    "        channel_id, \n",
    "        video_id, \n",
    "        title, \n",
    "        tags, \n",
    "        thumbnail, \n",
    "        description, \n",
    "        publication_timestamp, \n",
    "        duration, \n",
    "        views, \n",
    "        likes, \n",
    "        comments, \n",
    "        favorite_count, \n",
    "        definition, \n",
    "        caption_status\n",
    "        )\n",
    "        VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        '''\n",
    "        \n",
    "        values = (\n",
    "            row['channel_name'], \n",
    "            row['channel_id'], \n",
    "            row['video_id'], \n",
    "            row['title'], \n",
    "            tags, \n",
    "            row['thumbnail'], \n",
    "            row['description'], \n",
    "            publication_timestamp, \n",
    "            duration, \n",
    "            row['views'], \n",
    "            row['likes'], \n",
    "            row['comments'], \n",
    "            row['favorite_count'], \n",
    "            row['definition'], \n",
    "            row['caption_status']\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            cursor.execute(insert_query, values)\n",
    "            mydb.commit()\n",
    "            \n",
    "        except mysql.connector.Error as err:\n",
    "            if err.errno == 1062:\n",
    "                print('Video values already inserted')\n",
    "            \n",
    "            else:\n",
    "                print(f'Error: {err}')\n",
    "        \n",
    "        \n",
    "#videos_table(cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c522e4",
   "metadata": {},
   "source": [
    "# Comments table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "942f7c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comments_table(cursor):\n",
    "    drop_query = 'DROP TABLE IF EXISTS comments'\n",
    "    cursor.execute(drop_query)\n",
    "    mydb.commit()\n",
    "    \n",
    "    try:\n",
    "        create_query = '''\n",
    "        CREATE TABLE IF NOT EXISTS comments(\n",
    "        comment_id VARCHAR(100) PRIMARY KEY, \n",
    "        video_id VARCHAR(80), \n",
    "        comment_text TEXT, \n",
    "        comment_author VARCHAR(150), \n",
    "        comment_timestamp TIMESTAMP\n",
    "        )\n",
    "        '''\n",
    "        cursor.execute(create_query)\n",
    "        mydb.commit()\n",
    "        \n",
    "    except mysql.connector.Error as err:\n",
    "        if err.errno == 1062:\n",
    "            print('Comments table already created')\n",
    "            \n",
    "        else:\n",
    "            print(f'Error: {err}')\n",
    "            \n",
    "    db = client['Youtube_data']\n",
    "    coll1 = db['channel_details']\n",
    "    cmnt_lists = []\n",
    "    for cmnt_list in coll1.find({}, {'_id': 0, 'comment_information': 1}):\n",
    "        cmnt_list_data = cmnt_list['comment_information']\n",
    "        for _ in range(len(cmnt_list_data)):\n",
    "            cmnt_lists.append(cmnt_list_data[_])\n",
    "            \n",
    "    df = pd.DataFrame(cmnt_lists)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        comment_timestamp = date_formatter(row['comment_timestamp'])\n",
    "        insert_query = '''\n",
    "        INSERT INTO comments(\n",
    "        comment_id, \n",
    "        video_id, \n",
    "        comment_text, \n",
    "        comment_author, \n",
    "        comment_timestamp\n",
    "        )\n",
    "        VALUES(%s, %s, %s, %s, %s)\n",
    "        '''\n",
    "        \n",
    "        values = (\n",
    "            row['comment_id'], \n",
    "            row['video_id'], \n",
    "            row['comment_text'], \n",
    "            row['comment_author'], \n",
    "            comment_timestamp\n",
    "        )\n",
    "        try:\n",
    "            cursor.execute(insert_query, values)\n",
    "            mydb.commit()\n",
    "            \n",
    "        except mysql.connector.Error as err:\n",
    "            if err.errno == 1062:\n",
    "                print('Comment values already inserted')\n",
    "                \n",
    "            else:\n",
    "                print(f'Error: {err}')\n",
    "        \n",
    "        \n",
    "#comments_table(cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a542d5",
   "metadata": {},
   "source": [
    "# All tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69cc4db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All tables created successfully!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tables(cursor):\n",
    "    channels_table(cursor)\n",
    "    playlists_table(cursor)\n",
    "    videos_table(cursor)\n",
    "    comments_table(cursor)\n",
    "    return 'All tables created successfully!'\n",
    "    \n",
    "    \n",
    "tables(cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdd0c8f",
   "metadata": {},
   "source": [
    "# STREAMLIT WEB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04b61199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB:)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 04:05:31.340 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\Enigma\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "# IMPORTING MODULES\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from pymongo import MongoClient \n",
    "from pymongo.server_api import ServerApi\n",
    "import mysql.connector\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# MAKING CONNECTION\n",
    "def make_connection():\n",
    "    api_key = 'AIzaSyAjIU6fTQfKcnSsUkJ3-W-dQS8JBAGAmrU'\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    return youtube\n",
    "\n",
    "\n",
    "youtube = make_connection()\n",
    "\n",
    "# GATHERING CHANNEL INFORMATION\n",
    "def retrieve_channel_info(channel_id):\n",
    "    request = youtube.channels().list(\n",
    "        part = 'snippet, ContentDetails, Statistics', \n",
    "        id = channel_id, \n",
    "        maxResults = 50\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    for i in range(0, len(response['items'])):\n",
    "        data = {\n",
    "            'channel_name' : response['items'][i]['snippet']['title'], \n",
    "            'channel_id' : response['items'][i]['id'], \n",
    "            'subscription_count' : response['items'][i]['statistics']['subscriberCount'], \n",
    "            'views' : response['items'][i]['statistics']['viewCount'], \n",
    "            'videos_count' : response['items'][i]['statistics']['videoCount'], \n",
    "            'channel_description' : response['items'][i]['snippet']['description'], \n",
    "            'playlist_id' : response['items'][i]['contentDetails']['relatedPlaylists']['uploads']\n",
    "        }\n",
    "    \n",
    "    return data\n",
    "\n",
    "# GATHERING PLAYLIST INFORMATION\n",
    "def retrieve_playlist_info(channel_id):\n",
    "    playlist_data = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.playlists().list(\n",
    "                part='snippet, contentDetails',\n",
    "                channelId=channel_id,\n",
    "                pageToken=next_page_token,\n",
    "                maxResults=50,\n",
    "            )\n",
    "            response = request.execute()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error making API request: {e}\")\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            data = {\n",
    "                'playlist_id': item['id'],\n",
    "                'title': item['snippet']['title'],\n",
    "                'channel_id': item['snippet']['channelId'],\n",
    "                'channel_name': item['snippet']['channelTitle'],\n",
    "                'publication_timestamp': item['snippet']['publishedAt'],\n",
    "                'video_count': item['contentDetails']['itemCount']\n",
    "            }\n",
    "            playlist_data.append(data)\n",
    "        \n",
    "        next_page_token = response.get('nextPageToken')\n",
    "\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return playlist_data\n",
    "\n",
    "# GATHERING VIDEOIDS\n",
    "def retrieve_video_ids(channel_id):\n",
    "    retrieve_playlistid = retrieve_channel_info(channel_id)\n",
    "    \n",
    "    playlist_id =  retrieve_playlistid['playlist_id']\n",
    "        \n",
    "    next_page_token = None\n",
    "    video_ids = []\n",
    "    \n",
    "    while True:\n",
    "        request = youtube.playlistItems().list(\n",
    "            part = 'snippet', \n",
    "            playlistId = playlist_id, \n",
    "            pageToken = next_page_token, \n",
    "            maxResults = 50\n",
    "        )\n",
    "        \n",
    "        response = request.execute()\n",
    "        \n",
    "        for i in range(len(response['items'])):\n",
    "            video_ids.append(response['items'][i]['snippet']['resourceId']['videoId'])\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        \n",
    "        if not next_page_token:\n",
    "            break\n",
    "            \n",
    "    return video_ids\n",
    "\n",
    "# GATHERING VIDEO INFORMATION\n",
    "def retrieve_video_info(video_ids):\n",
    "    video_data = []\n",
    "    \n",
    "    for video_id in video_ids:\n",
    "\n",
    "        request = youtube.videos().list(\n",
    "            part = 'snippet, contentDetails, statistics', \n",
    "            id = video_id\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response['items']:\n",
    "            data = {\n",
    "                'channel_name' : item['snippet']['channelTitle'], \n",
    "                'channel_id' : item['snippet']['channelId'], \n",
    "                'video_id' : item['id'], \n",
    "                'title' : item['snippet']['title'], \n",
    "                'tags' : item['snippet'].get('tags', []), \n",
    "                'thumbnail' : item['snippet']['thumbnails']['default']['url'], \n",
    "                'description' : item['snippet']['description'], \n",
    "                'publication_timestamp' : item['snippet']['publishedAt'], \n",
    "                'duration' : item['contentDetails']['duration'], \n",
    "                'views' : item['statistics']['viewCount'], \n",
    "                'likes' : item['statistics'].get('likeCount'), \n",
    "                'comments' : item['statistics'].get('commentCount'), \n",
    "                'favorite_count' : item['statistics']['favoriteCount'], \n",
    "                'definition' : item['contentDetails']['definition'], \n",
    "                'caption_status' : item['contentDetails']['caption']\n",
    "            }\n",
    "            video_data.append(data)\n",
    "\n",
    "\n",
    "    return video_data\n",
    "\n",
    "# GATHERING COMMENT INFORMATION\n",
    "def retrieve_comment_info(video_ids):\n",
    "    comment_data = []\n",
    "    \n",
    "    try:\n",
    "        for video_id in video_ids:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part = 'snippet', \n",
    "                videoId = video_id, \n",
    "                maxResults = 50\n",
    "            )\n",
    "            response = request.execute()\n",
    "            \n",
    "            for item in response['items']:\n",
    "                data = {\n",
    "                    'comment_id' : item['snippet']['topLevelComment']['id'], \n",
    "                    'video_id' : item['snippet']['videoId'], \n",
    "                    'comment_text' : item['snippet']['topLevelComment']['snippet']['textOriginal'], \n",
    "                    'comment_author' : item['snippet']['topLevelComment']['snippet']['authorDisplayName'], \n",
    "                    'comment_timestamp' : item['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "                }\n",
    "                \n",
    "                comment_data.append(data)\n",
    "                \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return comment_data\n",
    "\n",
    "# CONNECTING MONGODB ATLAS\n",
    "uri = 'mongodb+srv://wolfx:wolfxkills@cluster0.hszoe9p.mongodb.net/?retryWrites=true&w=majority'\n",
    "    \n",
    "client = MongoClient(uri, server_api = ServerApi('1'), tz_aware = False, connect = True)\n",
    "\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print('Pinged your deployment. You successfully connected to MongoDB:)')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'Connection failed due to {e}')\n",
    "    \n",
    "db = client['Youtube_data']\n",
    "\n",
    "# STORING DATA IN MONGODB\n",
    "def channel_details(channel_id):\n",
    "    chl_details = retrieve_channel_info(channel_id)\n",
    "    plylst_details = retrieve_playlist_info(channel_id)\n",
    "    vdo_ids = retrieve_video_ids(channel_id)\n",
    "    vdo_details = retrieve_video_info(vdo_ids)\n",
    "    comnt_details = retrieve_comment_info(vdo_ids)\n",
    "    \n",
    "    coll1 = db['channel_details']\n",
    "    coll1.insert_one({\n",
    "        'channel_information' : chl_details, 'playlist_information' : plylst_details, 'video_information' : vdo_details, \n",
    "        'comment_information' : comnt_details\n",
    "    })\n",
    "    \n",
    "    return 'upload completed successfully'\n",
    "\n",
    "# MIGRATING DATA FROM MONGODB TO SQL\n",
    "def date_formatter(published_time_yt):\n",
    "    parsed_timestamp = datetime.strptime(published_time_yt, '%Y-%m-%dT%H:%M:%SZ')\n",
    "    formatted_timestamp = parsed_timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return formatted_timestamp\n",
    "\n",
    "def duration_formatter(duration):\n",
    "    duration = duration[2:]\n",
    "    parsed_duration = timedelta()\n",
    "    time_part = ''\n",
    "    \n",
    "    for char in duration:\n",
    "        if char.isdigit():\n",
    "            time_part += char\n",
    "            \n",
    "        else:\n",
    "            if char == 'H':\n",
    "                parsed_duration += timedelta(hours = int(time_part))\n",
    "            elif char == 'M':\n",
    "                parsed_duration += timedelta(minutes = int(time_part))\n",
    "            elif char == 'S':\n",
    "                parsed_duration += timedelta(seconds = int(time_part))\n",
    "            time_part = ''\n",
    "            \n",
    "    return parsed_duration\n",
    "\n",
    "# CHANNEL TABLE\n",
    "def channels_table(cursor):\n",
    "    drop_query = 'DROP TABLE IF EXISTS channels'\n",
    "    cursor.execute(drop_query)\n",
    "    mydb.commit()\n",
    "\n",
    "    try:\n",
    "        create_query = '''\n",
    "        CREATE TABLE IF NOT EXISTS channels(\n",
    "        channel_name VARCHAR(100), \n",
    "        channel_id VARCHAR(80) PRIMARY KEY, \n",
    "        subscription_count BIGINT, \n",
    "        views BIGINT, \n",
    "        videos_count INT, \n",
    "        channel_description TEXT, \n",
    "        playlist_id VARCHAR(50)\n",
    "        )\n",
    "        '''\n",
    "\n",
    "        cursor.execute(create_query)\n",
    "        mydb.commit()\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        if err.errno == 1062:\n",
    "            print('channels tables already created')\n",
    "\n",
    "        else:\n",
    "            print(f'Error: {err}')\n",
    "\n",
    "    db = client['Youtube_data']\n",
    "    coll1 = db['channel_details']\n",
    "    ch_list = []\n",
    "\n",
    "    for ch_data in coll1.find({}, {'_id': 0, 'channel_information': 1}):\n",
    "        ch_list.append(ch_data['channel_information'])\n",
    "        \n",
    "    df = pd.DataFrame(ch_list)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        insert_query = '''\n",
    "        INSERT INTO channels(\n",
    "        channel_name, \n",
    "        channel_id, \n",
    "        subscription_count, \n",
    "        views, \n",
    "        videos_count, \n",
    "        channel_description, \n",
    "        playlist_id\n",
    "        )\n",
    "        VALUES(%s, %s, %s, %s, %s, %s, %s)\n",
    "        '''\n",
    "        \n",
    "        values = (\n",
    "            row['channel_name'], \n",
    "            row['channel_id'], \n",
    "            row['subscription_count'], \n",
    "            row['views'], \n",
    "            row['videos_count'], \n",
    "            row['channel_description'], \n",
    "            row['playlist_id']\n",
    "        )\n",
    "        \n",
    "        try: \n",
    "            cursor.execute(insert_query, values)\n",
    "            mydb.commit()\n",
    "            \n",
    "        except mysql.connector.Error as err:\n",
    "            if err.errno == 1062:  # MySQL duplicate entry code\n",
    "                print('Channels values already inserted')\n",
    "            \n",
    "            else :\n",
    "                print(f'Error: {err}')\n",
    "                \n",
    "# PLAYLIST TABLE\n",
    "def playlists_table(cursor):\n",
    "    drop_query = 'DROP TABLE IF EXISTS playlists'\n",
    "    cursor.execute(drop_query)\n",
    "    mydb.commit()\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        create_query = '''\n",
    "        CREATE TABLE IF NOT EXISTS playlists(\n",
    "        playlist_id VARCHAR(100) PRIMARY KEY, \n",
    "        title VARCHAR(80), \n",
    "        channelid VARCHAR(100), \n",
    "        channelname VARCHAR(100), \n",
    "        publication_timestamp TIMESTAMP, \n",
    "        video_count INT\n",
    "        )\n",
    "        '''\n",
    "        cursor.execute(create_query)\n",
    "        mydb.commit()\n",
    "        \n",
    "    except mysql.connector.Error as err:\n",
    "        if err.errno == 1062:\n",
    "            print('Playlists values already inserted')\n",
    "            \n",
    "        else :\n",
    "            print(f'Error: {err}')\n",
    "        \n",
    "    db = client['Youtube_data']\n",
    "    coll1 = db['channel_details']\n",
    "    ply_lists = []\n",
    "    for ply_list in coll1.find({}, {'_id': 0, 'playlist_information': 1}):\n",
    "        ply_list_data = ply_list['playlist_information']\n",
    "        for i in range(len(ply_list_data)):\n",
    "            ply_lists.append(ply_list_data[i])\n",
    "            \n",
    "    df = pd.DataFrame(ply_lists)\n",
    "    for index, row in df.iterrows():\n",
    "        publication_timestamp = date_formatter(row['publication_timestamp'])\n",
    "        \n",
    "        insert_query = '''\n",
    "        INSERT INTO playlists(\n",
    "        playlist_id, \n",
    "        title, \n",
    "        channelid, \n",
    "        channelname, \n",
    "        publication_timestamp, \n",
    "        video_count\n",
    "        )\n",
    "        VALUES(%s, %s, %s, %s, %s, %s)\n",
    "        '''\n",
    "        \n",
    "        values = (\n",
    "            row['playlist_id'], \n",
    "            row['title'], \n",
    "            row['channel_id'], \n",
    "            row['channel_name'], \n",
    "            publication_timestamp, \n",
    "            row['video_count']\n",
    "        )\n",
    "        try:\n",
    "            cursor.execute(insert_query, values)\n",
    "            mydb.commit()\n",
    "        \n",
    "        except mysql.connector.Error as err:\n",
    "            if err.errno == 1062:\n",
    "                print('playlists values already inserted')\n",
    "            else:\n",
    "                print(f'Error: {err}')\n",
    "                \n",
    "# VIDEO TABLE\n",
    "def videos_table(cursor):\n",
    "    drop_query = 'DROP TABLE IF EXISTS videos'\n",
    "    cursor.execute(drop_query)\n",
    "    mydb.commit()\n",
    "    \n",
    "    try:\n",
    "        create_query = '''\n",
    "        CREATE TABLE IF NOT EXISTS videos(\n",
    "        channel_name VARCHAR(100), \n",
    "        channel_id VARCHAR(80), \n",
    "        video_id VARCHAR(80) PRIMARY KEY, \n",
    "        title VARCHAR(150), \n",
    "        tags TEXT, \n",
    "        thumbnail VARCHAR(225), \n",
    "        description TEXT, \n",
    "        publication_timestamp TIMESTAMP, \n",
    "        duration TIME, \n",
    "        views BIGINT, \n",
    "        likes BIGINT, \n",
    "        comments INT, \n",
    "        favorite_count INT, \n",
    "        definition VARCHAR(20), \n",
    "        caption_status VARCHAR(50)\n",
    "        )\n",
    "        '''\n",
    "        \n",
    "        cursor.execute(create_query)\n",
    "        mydb.commit()\n",
    "        \n",
    "    except mysql.connector.Error as err:\n",
    "        if err.errno == 1062:\n",
    "            print('Videos table already created')\n",
    "            \n",
    "        else:\n",
    "            print(f'Error: {err}')\n",
    "            \n",
    "    db  = client['Youtube_data']\n",
    "    coll1 = db['channel_details']\n",
    "    vdo_lists = []\n",
    "    for vdo_list in coll1.find({}, {'_id': 0, 'video_information': 1}):\n",
    "        vdo_list_data = vdo_list['video_information']\n",
    "        for _ in range(len(vdo_list_data)):\n",
    "            vdo_lists.append(vdo_list_data[_])\n",
    "    \n",
    "    df = pd.DataFrame(vdo_lists)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        publication_timestamp = date_formatter(row['publication_timestamp'])\n",
    "        duration = duration_formatter(row['duration'])\n",
    "        tags = ', '.join(map(str, row['tags'])) if row['tags'] else None\n",
    "        \n",
    "        insert_query = '''\n",
    "        INSERT INTO videos(\n",
    "        channel_name, \n",
    "        channel_id, \n",
    "        video_id, \n",
    "        title, \n",
    "        tags, \n",
    "        thumbnail, \n",
    "        description, \n",
    "        publication_timestamp, \n",
    "        duration, \n",
    "        views, \n",
    "        likes, \n",
    "        comments, \n",
    "        favorite_count, \n",
    "        definition, \n",
    "        caption_status\n",
    "        )\n",
    "        VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        '''\n",
    "        \n",
    "        values = (\n",
    "            row['channel_name'], \n",
    "            row['channel_id'], \n",
    "            row['video_id'], \n",
    "            row['title'], \n",
    "            tags, \n",
    "            row['thumbnail'], \n",
    "            row['description'], \n",
    "            publication_timestamp, \n",
    "            duration, \n",
    "            row['views'], \n",
    "            row['likes'], \n",
    "            row['comments'], \n",
    "            row['favorite_count'], \n",
    "            row['definition'], \n",
    "            row['caption_status']\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            cursor.execute(insert_query, values)\n",
    "            mydb.commit()\n",
    "            \n",
    "        except mysql.connector.Error as err:\n",
    "            if err.errno == 1062:\n",
    "                print('Video values already inserted')\n",
    "            \n",
    "            else:\n",
    "                print(f'Error: {err}')\n",
    "                \n",
    "# COMMENT TABLE\n",
    "def comments_table(cursor):\n",
    "    drop_query = 'DROP TABLE IF EXISTS comments'\n",
    "    cursor.execute(drop_query)\n",
    "    mydb.commit()\n",
    "    \n",
    "    try:\n",
    "        create_query = '''\n",
    "        CREATE TABLE IF NOT EXISTS comments(\n",
    "        comment_id VARCHAR(100) PRIMARY KEY, \n",
    "        video_id VARCHAR(80), \n",
    "        comment_text TEXT, \n",
    "        comment_author VARCHAR(150), \n",
    "        comment_timestamp TIMESTAMP\n",
    "        )\n",
    "        '''\n",
    "        cursor.execute(create_query)\n",
    "        mydb.commit()\n",
    "        \n",
    "    except mysql.connector.Error as err:\n",
    "        if err.errno == 1062:\n",
    "            print('Comments table already created')\n",
    "            \n",
    "        else:\n",
    "            print(f'Error: {err}')\n",
    "            \n",
    "    db = client['Youtube_data']\n",
    "    coll1 = db['channel_details']\n",
    "    cmnt_lists = []\n",
    "    for cmnt_list in coll1.find({}, {'_id': 0, 'comment_information': 1}):\n",
    "        cmnt_list_data = cmnt_list['comment_information']\n",
    "        for _ in range(len(cmnt_list_data)):\n",
    "            cmnt_lists.append(cmnt_list_data[_])\n",
    "            \n",
    "    df = pd.DataFrame(cmnt_lists)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        comment_timestamp = date_formatter(row['comment_timestamp'])\n",
    "        insert_query = '''\n",
    "        INSERT INTO comments(\n",
    "        comment_id, \n",
    "        video_id, \n",
    "        comment_text, \n",
    "        comment_author, \n",
    "        comment_timestamp\n",
    "        )\n",
    "        VALUES(%s, %s, %s, %s, %s)\n",
    "        '''\n",
    "        \n",
    "        values = ( \n",
    "            row['comment_id'], \n",
    "            row['video_id'], \n",
    "            row['comment_text'], \n",
    "            row['comment_author'], \n",
    "            comment_timestamp\n",
    "        )\n",
    "        try:\n",
    "            cursor.execute(insert_query, values)\n",
    "            mydb.commit()\n",
    "            \n",
    "        except mysql.connector.Error as err:\n",
    "            if err.errno == 1062:\n",
    "                print('Comment values already inserted')\n",
    "                \n",
    "            else:\n",
    "                print(f'Error: {err}')\n",
    "                \n",
    "\n",
    "# RETRIEVING INFORMATION FROM MONGODB\n",
    "mydb = mysql.connector.connect(\n",
    "            host = 'localhost', \n",
    "            user = 'root', \n",
    "            password = 'Abu#@7899#', \n",
    "            database = 'youtube_data', \n",
    "            port = '3306'\n",
    "        )\n",
    "\n",
    "cursor = mydb.cursor()\n",
    "    \n",
    "def channels_table_mdb():\n",
    "    db = client['Youtube_data']\n",
    "    coll1 = db['channel_details']\n",
    "    ch_list = []\n",
    "\n",
    "    for ch_data in coll1.find({}, {'_id': 0, 'channel_information': 1}):\n",
    "        ch_list.append(ch_data['channel_information'])\n",
    "        \n",
    "    df = st.dataframe(ch_list)\n",
    "    return df\n",
    "\n",
    "def playlists_table_mdb():\n",
    "    db = client['Youtube_data']\n",
    "    coll1 = db['channel_details']\n",
    "    ply_lists = []\n",
    "    for ply_list in coll1.find({}, {'_id': 0, 'playlist_information': 1}):\n",
    "        ply_list_data = ply_list['playlist_information']\n",
    "        for i in range(len(ply_list_data)):\n",
    "            ply_lists.append(ply_list_data[i])\n",
    "            \n",
    "    df = st.dataframe(ply_lists)\n",
    "    return df\n",
    "\n",
    "def videos_table_mdb():\n",
    "    db  = client['Youtube_data']\n",
    "    coll1 = db['channel_details']\n",
    "    vdo_lists = []\n",
    "    for vdo_list in coll1.find({}, {'_id': 0, 'video_information': 1}):\n",
    "        vdo_list_data = vdo_list['video_information']\n",
    "        for _ in range(len(vdo_list_data)):\n",
    "            vdo_lists.append(vdo_list_data[_])\n",
    "    \n",
    "    df = st.dataframe(vdo_lists)\n",
    "    return df\n",
    "\n",
    "def comments_table_mdb():\n",
    "    db = client['Youtube_data']\n",
    "    coll1 = db['channel_details']\n",
    "    cmnt_lists = []\n",
    "    for cmnt_list in coll1.find({}, {'_id': 0, 'comment_information': 1}):\n",
    "        cmnt_list_data = cmnt_list['comment_information']\n",
    "        for _ in range(len(cmnt_list_data)):\n",
    "            cmnt_lists.append(cmnt_list_data[_])\n",
    "            \n",
    "    df = st.dataframe(cmnt_lists)\n",
    "    return df\n",
    "\n",
    "                \n",
    "# CALLING ALLTABLES                \n",
    "def tables(cursor):\n",
    "    channels_table(cursor)\n",
    "    playlists_table(cursor)\n",
    "    videos_table(cursor)\n",
    "    comments_table(cursor)\n",
    "    return 'All tables created successfully!'\n",
    "\n",
    "\n",
    "#STREAMLIT\n",
    "img1 = Image.open('Image\\\\yt_page.png')\n",
    "st.set_page_config(page_title = 'Youtube Project', page_icon = img1, layout = 'wide')\n",
    "st.title(':red[Youtube Data Harvesting and Warehousing]')\n",
    "\n",
    "#NAVIGATION\n",
    "home, search, data_insights = st.tabs(\n",
    "    ['Home', 'Gathering Data', 'Data Insights']\n",
    ")\n",
    "\n",
    "with home:\n",
    "    img2 = Image.open('Image\\\\yt.png')\n",
    "    st.image(img2, width = 200)\n",
    "    st.write('''YouTube is an American online video sharing and social media platform\n",
    "    owned by Google. Accessible worldwide,[7] it was launched on February 14, 2005, \n",
    "    by Steve Chen, Chad Hurley, and Jawed Karim, three former employees of PayPal. \n",
    "    Headquartered in San Bruno, California, United States, it is the second most \n",
    "    visited website in the world, after Google Search. \n",
    "    '''\n",
    "    )\n",
    "    st.header('About Project')\n",
    "    st.write('''\n",
    "    This project aims to develop a user-friendly Streamlit application that utilizes the\n",
    "    Google API to extract information from a YouTube channel using channelid, stores it in a \n",
    "    MongoDB database, migrates it to a SQL data warehouse, and enables users to find useful \n",
    "    insights from the data collected.\n",
    "    '''\n",
    "    )\n",
    "    \n",
    "with search:\n",
    "    channel_id = st.text_input('Enter Channel Id')\n",
    "    channels = channel_id.split(',')\n",
    "    channels = [ch.strip() for ch in channels if ch]\n",
    "    \n",
    "    if st.button('Gather data', type = 'primary'):\n",
    "        for channel in channels:\n",
    "            ch_ids = []\n",
    "            db = client['Youtube_data']\n",
    "            coll1 = db['channel_details']\n",
    "            for ch_lst in coll1.find({}, {'_id': 0, 'channel_information': 1}):\n",
    "                ch_ids.append(ch_lst['channel_information']['channel_id'])\n",
    "            if channel in ch_ids:\n",
    "                st.success(f'Given Channel id {channel} already exists! please proceed further')\n",
    "            \n",
    "            else:\n",
    "                output = channel_details(channel)\n",
    "                st.success(output)\n",
    "            \n",
    "    if st.button(':red[Migrate data to sql]'):\n",
    "        display = tables(cursor)\n",
    "        st.success(display)\n",
    "        \n",
    "    show_table = st.selectbox(':black[Select tables]', \n",
    "                              ('Select an option', 'channels', 'playlists', 'videos', 'comments')\n",
    "    )\n",
    "    \n",
    "    if show_table == 'channels':\n",
    "        channels_table_mdb()\n",
    "    \n",
    "    elif show_table == 'playlists':\n",
    "        playlists_table_mdb()\n",
    "    \n",
    "    elif show_table == 'videos':\n",
    "        videos_table_mdb()\n",
    "        \n",
    "    elif show_table == 'comments':\n",
    "        comments_table_mdb()\n",
    "    \n",
    "with data_insights:\n",
    "    st.markdown(':red[Youtube Data]')    \n",
    "\n",
    "    question = st.selectbox(\n",
    "    ':red[How would you prefer to view the gathered information?]', (\n",
    "        '1.All videos and their corresponding channels', \n",
    "        '2.Channels with most number of videos', \n",
    "        '3.Top 10 videos', \n",
    "        '4.Comments made on each video', \n",
    "        '5.Highly liked videos', \n",
    "        '6.Likes made on each video', \n",
    "        '7.Total views of each channels',\n",
    "        '8.Channels published videos on 2022', \n",
    "        '9.Average video duration of each channel', \n",
    "        '10.Highly commented videos'\n",
    "    ), \n",
    "        index = None, \n",
    "        placeholder = 'Select your question',\n",
    "    )\n",
    "\n",
    "    if question == '1.All videos and their corresponding channels':\n",
    "        query = '''SELECT channel_name, title FROM videos'''\n",
    "        cursor.execute(query)\n",
    "        tuples = cursor.fetchall()\n",
    "        df = pd.DataFrame(tuples, columns = ['Channel Name', 'Video Title'])\n",
    "        fig = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df.columns),\n",
    "                    fill_color='red',\n",
    "                    font = dict(color = 'white', size=15),\n",
    "                    align=['left', 'center']),\n",
    "        cells=dict(values=[df['Channel Name'], df['Video Title']],\n",
    "                   fill_color='white',\n",
    "                   font = dict(color = 'black', size=13),\n",
    "                   align=['left', 'center']))\n",
    "                             ]\n",
    "                       )\n",
    "        fig.update_layout(width=1225, height=700)\n",
    "        st.write(fig)\n",
    "\n",
    "    elif question == '2.Channels with most number of videos':\n",
    "        query = '''SELECT channel_name, videos_count FROM channels ORDER BY videos_count DESC''' #DESC-descending\n",
    "        cursor.execute(query)\n",
    "        tuples = cursor.fetchall()\n",
    "        df = pd.DataFrame(tuples, columns = ['Channel Name', 'Videos Count'])\n",
    "        fig = px.bar(df, x = 'Channel Name', y = 'Videos Count',\n",
    "                     hover_data = ['Channel Name', 'Videos Count'],\n",
    "                     color = 'Videos Count',\n",
    "                     color_continuous_scale = 'reds',\n",
    "                     width = 1200,\n",
    "                     height = 700,\n",
    "                     title = 'High Videos Channel',\n",
    "                     text_auto = True\n",
    "                    )\n",
    "        st.plotly_chart(fig)\n",
    "\n",
    "    elif question == '3.Top 10 videos':\n",
    "        query = '''SELECT channel_name, title, views FROM videos \n",
    "        WHERE views IS NOT NULL ORDER BY views DESC LIMIT 10\n",
    "        '''\n",
    "        cursor.execute(query)\n",
    "        tuples = cursor.fetchall()\n",
    "        df = pd.DataFrame(tuples, columns = ['Channel Name', 'Video Name', 'View Count'])\n",
    "        fig = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df.columns),\n",
    "                    fill_color='red',\n",
    "                    font = dict(color = 'white', size=15),\n",
    "                    align=['left', 'center']),\n",
    "        cells=dict(values=[df['Channel Name'], df['Video Name'], df['View Count']],\n",
    "                   fill_color='white',\n",
    "                   font = dict(color = 'black', size=13),\n",
    "                   align=['left', 'center']))\n",
    "                             ]\n",
    "                       )\n",
    "        fig.update_layout(width=1225, height=700)\n",
    "        st.write(fig)\n",
    "\n",
    "    elif question == '4.Comments made on each video':\n",
    "        query = '''SELECT title, comments FROM videos WHERE comments IS NOT NULL'''\n",
    "        cursor.execute(query)\n",
    "        tuples = cursor.fetchall()\n",
    "        df = pd.DataFrame(tuples, columns = ['Video Name', 'Comments Count'])\n",
    "        fig = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df.columns),\n",
    "                    fill_color='red',\n",
    "                    font = dict(color = 'white', size=15),\n",
    "                    align=['left', 'center']),\n",
    "        cells=dict(values=[df['Video Name'], df['Comments Count']],\n",
    "                   fill_color='white',\n",
    "                   font = dict(color = 'black', size=13),\n",
    "                   align=['left', 'center']))\n",
    "                             ]\n",
    "                       )\n",
    "        fig.update_layout(width=1225, height=700)\n",
    "        st.write(fig)\n",
    "\n",
    "    elif question == '5.Highly liked videos':\n",
    "        query = '''SELECT channel_name, title, likes FROM videos \n",
    "        WHERE likes IS NOT NULL ORDER BY likes DESC\n",
    "        '''\n",
    "        cursor.execute(query)\n",
    "        tuples = cursor.fetchall()\n",
    "        df = pd.DataFrame(tuples, columns = ['Channel Name', 'Video Name', 'Likes Count'])\n",
    "        fig = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df.columns),\n",
    "                    fill_color='red',\n",
    "                    font = dict(color = 'white', size=15),\n",
    "                    align=['left', 'center']),\n",
    "        cells=dict(values=[df['Channel Name'], df['Video Name'], df['Likes Count']],\n",
    "                   fill_color='white',\n",
    "                   font = dict(color = 'black', size=13),\n",
    "                   align=['left', 'center']))\n",
    "                             ]\n",
    "                       )\n",
    "        fig.update_layout(width=1225, height=700)\n",
    "        st.write(fig)\n",
    "\n",
    "    elif question == '6.Likes made on each video':\n",
    "        query = '''SELECT title, likes FROM videos'''\n",
    "        cursor.execute(query)\n",
    "        tuples = cursor.fetchall()\n",
    "        df = pd.DataFrame(tuples, columns = ['Video Name', 'Likes Count'])\n",
    "        fig = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df.columns),\n",
    "                    fill_color='red',\n",
    "                    font = dict(color = 'white', size=15),\n",
    "                    align=['left', 'center']),\n",
    "        cells=dict(values=[df['Video Name'], df['Likes Count']],\n",
    "                   fill_color='white',\n",
    "                   font = dict(color = 'black', size=13),\n",
    "                   align=['left', 'center']))\n",
    "                             ]\n",
    "                       )\n",
    "        fig.update_layout(width=1225, height=700)\n",
    "        st.write(fig)\n",
    "\n",
    "    elif question == '7.Total views of each channels':\n",
    "        query = '''SELECT channel_name, views FROM channels'''\n",
    "        cursor.execute(query)\n",
    "        tuples = cursor.fetchall()\n",
    "        df = pd.DataFrame(tuples, columns = ['Channel Name', 'Views Count'])\n",
    "        fig = px.bar(df, x = 'Channel Name', y = 'Views Count',\n",
    "                     hover_data = ['Channel Name', 'Views Count'],\n",
    "                     color = 'Views Count',\n",
    "                     color_continuous_scale = 'reds',\n",
    "                     width = 1200,\n",
    "                     height = 700,\n",
    "                     title = 'Total Views of Channels',\n",
    "                     text_auto = True, \n",
    "                    )\n",
    "        st.plotly_chart(fig)\n",
    "\n",
    "    elif question == '8.Channels published videos on 2022':\n",
    "        query = '''SELECT channel_name, title, publication_timestamp FROM videos \n",
    "        WHERE EXTRACT(YEAR FROM publication_timestamp) = 2022\n",
    "        '''\n",
    "        cursor.execute(query)\n",
    "        tuples = cursor.fetchall()\n",
    "        df = pd.DataFrame(tuples, columns = ['Channel Name', 'Video Name', 'Publication Timestamp'])\n",
    "        fig = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df.columns),\n",
    "                    fill_color='red',\n",
    "                    font = dict(color = 'white', size=15),\n",
    "                    align=['left', 'center']),\n",
    "        cells=dict(values=[df['Channel Name'], df['Video Name'], df['Publication Timestamp']],\n",
    "                   fill_color='white',\n",
    "                   font = dict(color = 'black', size=13),\n",
    "                   align=['left', 'center']))\n",
    "                             ],\n",
    "                       )\n",
    "        fig.update_layout(width=1225, height=700)\n",
    "        st.write(fig)\n",
    "\n",
    "    elif question == '9.Average video duration of each channel':\n",
    "        query = '''SELECT channel_name, AVG(TIME_TO_SEC(duration))/60 FROM videos GROUP BY channel_name'''\n",
    "        cursor.execute(query)\n",
    "        tuples = cursor.fetchall()\n",
    "        df = pd.DataFrame(tuples, columns = ['Channel Name', 'Average Duration'])\n",
    "        df['Average Duration'] = pd.to_numeric(df['Average Duration'])\n",
    "        fig = px.bar(df, x = 'Channel Name', y = 'Average Duration',\n",
    "                     hover_data = ['Channel Name', 'Average Duration'],\n",
    "                     color = 'Average Duration',\n",
    "                     color_continuous_scale = 'reds',\n",
    "                     width = 1200,\n",
    "                     height = 700,\n",
    "                     title = 'Average Duration of Channels (in Minutes)',\n",
    "                     text_auto = True, \n",
    "                    )\n",
    "        st.plotly_chart(fig)\n",
    "\n",
    "    elif question == '10.Highly commented videos':\n",
    "        query = '''SELECT channel_name, title, comments FROM videos \n",
    "        WHERE comments IS NOT NULL ORDER BY comments DESC\n",
    "        '''\n",
    "        cursor.execute(query)\n",
    "        tuples = cursor.fetchall()\n",
    "        df = pd.DataFrame(tuples, columns = ['Channel Name', 'Video Name', 'Comments Count'])\n",
    "        fig = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df.columns),\n",
    "                    fill_color='red',\n",
    "                    font = dict(color = 'white', size=15),\n",
    "                    align=['left', 'center']),\n",
    "        cells=dict(values=[df['Channel Name'], df['Video Name'], df['Comments Count']],\n",
    "                   fill_color='white',\n",
    "                   font = dict(color = 'black', size=13),\n",
    "                   align=['left', 'center']))\n",
    "                             ],\n",
    "                       )\n",
    "        fig.update_layout(width=1225, height=700)\n",
    "        st.write(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7afd7350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Channel Name         object\n",
       "Average Duration    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = '''SELECT channel_name, TIME_TO_SEC(AVG(duration))/60 FROM videos GROUP BY channel_name'''\n",
    "cursor.execute(query)\n",
    "tuples = cursor.fetchall()\n",
    "df = pd.DataFrame(tuples, columns = ['Channel Name', 'Average Duration'])\n",
    "df['Average Duration'] = pd.to_numeric(df['Average Duration'])\n",
    "fig = px.bar(df, x='Channel Name', y='Average Duration',\n",
    "     title='Average Duration by Channel',\n",
    "     color='Channel Name',\n",
    "    )\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "042c1812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:37:02\n"
     ]
    }
   ],
   "source": [
    "print(timedelta(seconds = 2222))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3180bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "delta": {
          "reference": 380
         },
         "domain": {
          "x": [
           0,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "gauge": {
          "axis": {
           "range": [
            null,
            500
           ]
          },
          "steps": [
           {
            "color": "lightgray",
            "range": [
             0,
             250
            ]
           },
           {
            "color": "gray",
            "range": [
             250,
             400
            ]
           }
          ],
          "threshold": {
           "line": {
            "color": "red",
            "width": 4
           },
           "thickness": 0.75,
           "value": 490
          }
         },
         "mode": "gauge+number+delta",
         "title": {
          "text": "Speed"
         },
         "type": "indicator",
         "value": 5669675
        }
       ],
       "layout": {
        "template": {
         "data": {
          "candlestick": [
           {
            "decreasing": {
             "line": {
              "color": "#000033"
             }
            },
            "increasing": {
             "line": {
              "color": "#000032"
             }
            },
            "type": "candlestick"
           }
          ],
          "contour": [
           {
            "colorscale": [
             [
              0,
              "#000011"
             ],
             [
              0.1111111111111111,
              "#000012"
             ],
             [
              0.2222222222222222,
              "#000013"
             ],
             [
              0.3333333333333333,
              "#000014"
             ],
             [
              0.4444444444444444,
              "#000015"
             ],
             [
              0.5555555555555556,
              "#000016"
             ],
             [
              0.6666666666666666,
              "#000017"
             ],
             [
              0.7777777777777778,
              "#000018"
             ],
             [
              0.8888888888888888,
              "#000019"
             ],
             [
              1,
              "#000020"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorscale": [
             [
              0,
              "#000011"
             ],
             [
              0.1111111111111111,
              "#000012"
             ],
             [
              0.2222222222222222,
              "#000013"
             ],
             [
              0.3333333333333333,
              "#000014"
             ],
             [
              0.4444444444444444,
              "#000015"
             ],
             [
              0.5555555555555556,
              "#000016"
             ],
             [
              0.6666666666666666,
              "#000017"
             ],
             [
              0.7777777777777778,
              "#000018"
             ],
             [
              0.8888888888888888,
              "#000019"
             ],
             [
              1,
              "#000020"
             ]
            ],
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorscale": [
             [
              0,
              "#000011"
             ],
             [
              0.1111111111111111,
              "#000012"
             ],
             [
              0.2222222222222222,
              "#000013"
             ],
             [
              0.3333333333333333,
              "#000014"
             ],
             [
              0.4444444444444444,
              "#000015"
             ],
             [
              0.5555555555555556,
              "#000016"
             ],
             [
              0.6666666666666666,
              "#000017"
             ],
             [
              0.7777777777777778,
              "#000018"
             ],
             [
              0.8888888888888888,
              "#000019"
             ],
             [
              1,
              "#000020"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram2d": [
           {
            "colorscale": [
             [
              0,
              "#000011"
             ],
             [
              0.1111111111111111,
              "#000012"
             ],
             [
              0.2222222222222222,
              "#000013"
             ],
             [
              0.3333333333333333,
              "#000014"
             ],
             [
              0.4444444444444444,
              "#000015"
             ],
             [
              0.5555555555555556,
              "#000016"
             ],
             [
              0.6666666666666666,
              "#000017"
             ],
             [
              0.7777777777777778,
              "#000018"
             ],
             [
              0.8888888888888888,
              "#000019"
             ],
             [
              1,
              "#000020"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "icicle": [
           {
            "textfont": {
             "color": "white"
            },
            "type": "icicle"
           }
          ],
          "sankey": [
           {
            "textfont": {
             "color": "#000036"
            },
            "type": "sankey"
           }
          ],
          "scatter": [
           {
            "marker": {
             "line": {
              "width": 0
             }
            },
            "type": "scatter"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#000038"
             },
             "font": {
              "color": "#000037"
             },
             "line": {
              "color": "#000039"
             }
            },
            "header": {
             "fill": {
              "color": "#000040"
             },
             "font": {
              "color": "#000036"
             },
             "line": {
              "color": "#000039"
             }
            },
            "type": "table"
           }
          ],
          "waterfall": [
           {
            "connector": {
             "line": {
              "color": "#000036",
              "width": 2
             }
            },
            "decreasing": {
             "marker": {
              "color": "#000033"
             }
            },
            "increasing": {
             "marker": {
              "color": "#000032"
             }
            },
            "totals": {
             "marker": {
              "color": "#000034"
             }
            },
            "type": "waterfall"
           }
          ]
         },
         "layout": {
          "coloraxis": {
           "colorscale": [
            [
             0,
             "#000011"
            ],
            [
             0.1111111111111111,
             "#000012"
            ],
            [
             0.2222222222222222,
             "#000013"
            ],
            [
             0.3333333333333333,
             "#000014"
            ],
            [
             0.4444444444444444,
             "#000015"
            ],
            [
             0.5555555555555556,
             "#000016"
            ],
            [
             0.6666666666666666,
             "#000017"
            ],
            [
             0.7777777777777778,
             "#000018"
            ],
            [
             0.8888888888888888,
             "#000019"
            ],
            [
             1,
             "#000020"
            ]
           ]
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#000021"
            ],
            [
             0.1,
             "#000022"
            ],
            [
             0.2,
             "#000023"
            ],
            [
             0.3,
             "#000024"
            ],
            [
             0.4,
             "#000025"
            ],
            [
             0.5,
             "#000026"
            ],
            [
             0.6,
             "#000027"
            ],
            [
             0.7,
             "#000028"
            ],
            [
             0.8,
             "#000029"
            ],
            [
             0.9,
             "#000030"
            ],
            [
             1,
             "#000031"
            ]
           ],
           "sequential": [
            [
             0,
             "#000011"
            ],
            [
             0.1111111111111111,
             "#000012"
            ],
            [
             0.2222222222222222,
             "#000013"
            ],
            [
             0.3333333333333333,
             "#000014"
            ],
            [
             0.4444444444444444,
             "#000015"
            ],
            [
             0.5555555555555556,
             "#000016"
            ],
            [
             0.6666666666666666,
             "#000017"
            ],
            [
             0.7777777777777778,
             "#000018"
            ],
            [
             0.8888888888888888,
             "#000019"
            ],
            [
             1,
             "#000020"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#000011"
            ],
            [
             0.1111111111111111,
             "#000012"
            ],
            [
             0.2222222222222222,
             "#000013"
            ],
            [
             0.3333333333333333,
             "#000014"
            ],
            [
             0.4444444444444444,
             "#000015"
            ],
            [
             0.5555555555555556,
             "#000016"
            ],
            [
             0.6666666666666666,
             "#000017"
            ],
            [
             0.7777777777777778,
             "#000018"
            ],
            [
             0.8888888888888888,
             "#000019"
            ],
            [
             1,
             "#000020"
            ]
           ]
          },
          "colorway": [
           "#000001",
           "#000002",
           "#000003",
           "#000004",
           "#000005",
           "#000006",
           "#000007",
           "#000008",
           "#000009",
           "#000010"
          ]
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"a7c4d677-f629-4dde-81e3-c6556ef3930b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a7c4d677-f629-4dde-81e3-c6556ef3930b\")) {                    Plotly.newPlot(                        \"a7c4d677-f629-4dde-81e3-c6556ef3930b\",                        [{\"delta\":{\"reference\":380},\"domain\":{\"x\":[0,1],\"y\":[0,1]},\"gauge\":{\"axis\":{\"range\":[null,500]},\"steps\":[{\"color\":\"lightgray\",\"range\":[0,250]},{\"color\":\"gray\",\"range\":[250,400]}],\"threshold\":{\"line\":{\"color\":\"red\",\"width\":4},\"thickness\":0.75,\"value\":490}},\"mode\":\"gauge+number+delta\",\"title\":{\"text\":\"Speed\"},\"value\":5669675,\"type\":\"indicator\"}],                        {\"template\":{\"data\":{\"candlestick\":[{\"decreasing\":{\"line\":{\"color\":\"#000033\"}},\"increasing\":{\"line\":{\"color\":\"#000032\"}},\"type\":\"candlestick\"}],\"contourcarpet\":[{\"colorscale\":[[0.0,\"#000011\"],[0.1111111111111111,\"#000012\"],[0.2222222222222222,\"#000013\"],[0.3333333333333333,\"#000014\"],[0.4444444444444444,\"#000015\"],[0.5555555555555556,\"#000016\"],[0.6666666666666666,\"#000017\"],[0.7777777777777778,\"#000018\"],[0.8888888888888888,\"#000019\"],[1.0,\"#000020\"]],\"type\":\"contourcarpet\"}],\"contour\":[{\"colorscale\":[[0.0,\"#000011\"],[0.1111111111111111,\"#000012\"],[0.2222222222222222,\"#000013\"],[0.3333333333333333,\"#000014\"],[0.4444444444444444,\"#000015\"],[0.5555555555555556,\"#000016\"],[0.6666666666666666,\"#000017\"],[0.7777777777777778,\"#000018\"],[0.8888888888888888,\"#000019\"],[1.0,\"#000020\"]],\"type\":\"contour\"}],\"heatmap\":[{\"colorscale\":[[0.0,\"#000011\"],[0.1111111111111111,\"#000012\"],[0.2222222222222222,\"#000013\"],[0.3333333333333333,\"#000014\"],[0.4444444444444444,\"#000015\"],[0.5555555555555556,\"#000016\"],[0.6666666666666666,\"#000017\"],[0.7777777777777778,\"#000018\"],[0.8888888888888888,\"#000019\"],[1.0,\"#000020\"]],\"type\":\"heatmap\"}],\"histogram2d\":[{\"colorscale\":[[0.0,\"#000011\"],[0.1111111111111111,\"#000012\"],[0.2222222222222222,\"#000013\"],[0.3333333333333333,\"#000014\"],[0.4444444444444444,\"#000015\"],[0.5555555555555556,\"#000016\"],[0.6666666666666666,\"#000017\"],[0.7777777777777778,\"#000018\"],[0.8888888888888888,\"#000019\"],[1.0,\"#000020\"]],\"type\":\"histogram2d\"}],\"icicle\":[{\"textfont\":{\"color\":\"white\"},\"type\":\"icicle\"}],\"sankey\":[{\"textfont\":{\"color\":\"#000036\"},\"type\":\"sankey\"}],\"scatter\":[{\"marker\":{\"line\":{\"width\":0}},\"type\":\"scatter\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#000038\"},\"font\":{\"color\":\"#000037\"},\"line\":{\"color\":\"#000039\"}},\"header\":{\"fill\":{\"color\":\"#000040\"},\"font\":{\"color\":\"#000036\"},\"line\":{\"color\":\"#000039\"}},\"type\":\"table\"}],\"waterfall\":[{\"connector\":{\"line\":{\"color\":\"#000036\",\"width\":2}},\"decreasing\":{\"marker\":{\"color\":\"#000033\"}},\"increasing\":{\"marker\":{\"color\":\"#000032\"}},\"totals\":{\"marker\":{\"color\":\"#000034\"}},\"type\":\"waterfall\"}]},\"layout\":{\"coloraxis\":{\"colorscale\":[[0.0,\"#000011\"],[0.1111111111111111,\"#000012\"],[0.2222222222222222,\"#000013\"],[0.3333333333333333,\"#000014\"],[0.4444444444444444,\"#000015\"],[0.5555555555555556,\"#000016\"],[0.6666666666666666,\"#000017\"],[0.7777777777777778,\"#000018\"],[0.8888888888888888,\"#000019\"],[1.0,\"#000020\"]]},\"colorscale\":{\"diverging\":[[0.0,\"#000021\"],[0.1,\"#000022\"],[0.2,\"#000023\"],[0.3,\"#000024\"],[0.4,\"#000025\"],[0.5,\"#000026\"],[0.6,\"#000027\"],[0.7,\"#000028\"],[0.8,\"#000029\"],[0.9,\"#000030\"],[1.0,\"#000031\"]],\"sequential\":[[0.0,\"#000011\"],[0.1111111111111111,\"#000012\"],[0.2222222222222222,\"#000013\"],[0.3333333333333333,\"#000014\"],[0.4444444444444444,\"#000015\"],[0.5555555555555556,\"#000016\"],[0.6666666666666666,\"#000017\"],[0.7777777777777778,\"#000018\"],[0.8888888888888888,\"#000019\"],[1.0,\"#000020\"]],\"sequentialminus\":[[0.0,\"#000011\"],[0.1111111111111111,\"#000012\"],[0.2222222222222222,\"#000013\"],[0.3333333333333333,\"#000014\"],[0.4444444444444444,\"#000015\"],[0.5555555555555556,\"#000016\"],[0.6666666666666666,\"#000017\"],[0.7777777777777778,\"#000018\"],[0.8888888888888888,\"#000019\"],[1.0,\"#000020\"]]},\"colorway\":[\"#000001\",\"#000002\",\"#000003\",\"#000004\",\"#000005\",\"#000006\",\"#000007\",\"#000008\",\"#000009\",\"#000010\"]}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('a7c4d677-f629-4dde-81e3-c6556ef3930b');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    values = row['Views Count']\n",
    "    fig = go.Figure(go.Indicator(\n",
    "        domain = {'x': [0, 1], 'y': [0, 1]},\n",
    "        value = values,\n",
    "        mode = \"gauge+number+delta\",\n",
    "        title = {'text': \"Speed\"},\n",
    "        delta = {'reference': 380},\n",
    "        gauge = {'axis': {'range': [None, 500]},\n",
    "                 'steps' : [\n",
    "                     {'range': [0, 250], 'color': \"lightgray\"},\n",
    "                     {'range': [250, 400], 'color': \"gray\"}],\n",
    "                 'threshold' : {'line': {'color': \"red\", 'width': 4}, 'thickness': 0.75, 'value': 490}}))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "811b6d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "cells": {
          "align": "left",
          "fill": {
           "color": "lavender"
          },
          "values": [
           [
            "How to Build Your First Data Science Web App in Python - Streamlit Tutorial #1",
            "Build your first machine learning model in Python",
            "Bioinformatics Project from Scratch - Drug Discovery Part 1 (Data Collection and Pre-Processing)",
            "Web Apps in R: Building your First Web Application in R | Shiny Tutorial Ep 1",
            "The Art of Learning Data Science (How to learn data science)",
            "How to Do Data Cleaning (step-by-step tutorial on real-life dataset)",
            "How to Do Data Exploration (step-by-step tutorial on real-life dataset)",
            "How to Build a Simple Machine Learning Web App in Python - Streamlit Tutorial #2",
            "Streamlit Elements You Should Know About in 2023",
            "How to Build a Streamlit App (Beginner level Streamlit tutorial) - Part 1",
            "Machine Learning in Python: Building a Linear Regression Model",
            "Machine Learning for Drug Discovery (Explained in 2 minutes)",
            "How to Build Regression Models (Weka Tutorial #1)",
            "An Introduction to Computational Drug Discovery",
            "Easy Web Scraping in Python using Pandas for Data Science",
            "Building your Data Science Portfolio with GitHub (Data Science 101)",
            "Exploratory Data Analysis in Python using pandas",
            "Machine Learning in Python: Building a Classification Model",
            "Amazon's Machine Learning University (FREE Courses in Data Science)",
            "How to Implement Decision Trees in Python (Train, Test, Evaluate, Explain)",
            "Data Science 101: Deploying your Machine Learning Model",
            "Building a Dashboard web app in Python - Full Streamlit Tutorial",
            "FREE Online Courses in Data Science from Harvard University",
            "How a Biologist became a Data Scientist",
            "Strategies for Learning Data Science in 2020 (Data Science 101)",
            "How to Collect User Input with Streamlit - Part 2",
            "Data Science Virtual Internship - Part 1 (KPMG Data Analytics Consulting)",
            "How to Deploy Data Science Web App to Heroku | Streamlit #4",
            "How to Build Bioinformatics Tools",
            "How to Build a Machine Learning Model",
            "Exploratory Data Analysis in R: Towards Data Understanding",
            "My 3 go-to places to find interesting real-life datasets",
            "How to handle imbalanced datasets in Python",
            "How to Build a Penguin Classification Web App in Python | Streamlit #3",
            "Data Science for Computational Drug Discovery using Python (Part 1)",
            "How to Deploy a Streamlit App - Part 4",
            "Pandas functions: merge vs. join vs. concat",
            "How to Deploy Data Science Web App to Streamlit Sharing - Streamlit Tutorial #11",
            "How to write a great looking research article using LaTeX on Overleaf",
            "How to use the Pandas GroupBy function | Pandas tutorial",
            "Hyperparameter Tuning of Machine Learning Model in Python",
            "How to build your first simple web application in Python with PyWebIO",
            "Call for Participation in the Open Bioinformatics Research Project",
            "How to use Streamlit session states and callback functions | Make your apps remember things!",
            "Learn Pandas in 20 minutes!",
            "Pandas Functions: Apply vs. Map vs. Applymap",
            "Pandas Profiling for Data Science (Quick and Easy Exploratory Data Analysis)",
            "Practical Introduction to Google Colab for Data Science",
            "How to Quickly Perform Exploratory Data Analysis (EDA) in Python using Sweetviz",
            "How to stack machine learning models in Python",
            "How to use PandasGUI for Exploratory Data Analysis and Data Science",
            "How to Upload Files to Google Colab",
            "Ep. 1 How to create your first Streamlit web app in 1 minute #shorts",
            "How to Animate Plots on Streamlit, Bring your Plots to Life!",
            "Bioinformatics Project from Scratch - Drug Discovery Part 2 (Exploratory Data Analysis)",
            "Computational Drug Discovery: Machine Learning for Making Sense of Big Data in Drug Discovery",
            "How to Plot an ROC Curve in Python | Machine Learning in Python",
            "How to Save and Download files from Google Colab",
            "FREE Machine Learning Crash Course from Google",
            "Machine Learning in Python: Principal Component Analysis (PCA) for Handling High-Dimensional Data",
            "How to summarize text using ChatGPT",
            "Quick tour of PyCaret (a low-code machine learning library in Python)",
            "How to Make a Multi-Page Web App | Streamlit #16",
            "How to use the Llama 2 LLM in Python",
            "Data Science 101: Overview of Machine Learning Model Building Process",
            "How to Build a Dashboard Web App in Python with Streamlit",
            "How to build a machine learning model in Python from scratch",
            "Machine Learning in R: Building a Classification Model",
            "Save time with Pandas Grouper!",
            "How to Integrate Machine Learning to Streamlit - Part 3",
            "How to Replace Values of Dataframes | Replace, Where, Mask, Update and More",
            "How to Reshape Dataframes | Pivot, Stack, Melt and More",
            "How to Build Classification Models (Weka Tutorial #2)",
            "How to apply custom CSS styles in Streamlit apps",
            "How to build an Exploratory Data Analysis app using Pandas Profiling | Streamlit #19",
            "Ask Me Anything About Bioinformatics #1",
            "Pandas Functions: Three Ways to Use the Apply Function",
            "How to build a data science resume (portfolio website) in Python with Streamlit",
            "How to Build a Machine Learning App | Streamlit #13",
            "Web Apps in R: Build Interactive Histogram Web Application in R | Shiny Tutorial Ep 2",
            "The Data Science Process - A Visual Guide (Part 1)",
            "How to Make Tables in Streamlit Using Plotly",
            "How to Build a Football Data Web App in Python | Streamlit #9",
            "Data Science Portfolio Tips | Discussion with Ken Jee, Krish Naik, Codebasics and Data Professor",
            "How to Become a Data Scientist (Learning Path and Skill Sets Needed)",
            "Using Python in R",
            "Data Science for Bioinformatics",
            "Use native R on Google Colab for Data Science",
            "How to build a machine learning model to predict antimicrobial peptides (End-to-end Bioinformatics)",
            "Why do we split data into train test and validation sets?",
            "Learn Data Science for Free with Kaggle Micro-Courses",
            "Is it Possible to Add a Navigation Bar to Streamlit Apps? | Streamlit #29",
            "Bioinformatics Project from Scratch - Drug Discovery Part 3 (Dataset Preparation)",
            "Fastest way to upload files from Google Drive to Google Colab",
            "What programming language to learn for Data Science? R versus Python",
            "How to quickly explore data in Python using the D-Tale library",
            "Vaex - Fast data frame for Data Science (Handle billion rows in seconds)",
            "Compare Machine Learning Classifiers in Python",
            "Ep. 2 How to create a finance web app in 1 minute using Streamlit #shorts",
            "How to create your first data science project",
            "Papers With Code (Free Resource of Machine Learning Papers and Code)",
            "How to compare machine learning classifiers in 2 lines of code (lazypredict Python library)",
            "How to build a protein structure prediction app in Python using ESMFold and Streamlit",
            "Build a data storytelling web app in Python with ipyvizzu | Streamlit tutorial",
            "Build machine learning models in Google Sheets",
            "How to Build your First Classification Model using Visual Programming (Orange Tutorial #1)",
            "How to build a Stock price web app in Python | Streamlit #20",
            "How to Build a Stocks Price Web App in Python | Streamlit #10",
            "How to Build a Basketball Player Data Explorer Web App in Python | Streamlit #5",
            "This Book will Help you Land a Data Science Job",
            "How Do Decision Trees Work (Simple Explanation) - Learning and Training Process",
            "How to Speed Up Data Cleaning and Exploratory Data Analysis in Python with klib",
            "How to Build a Boston Housing Price Prediction Web App in Python | Streamlit #6",
            "How to Install and Use Pandas Profiling on Google Colab",
            "Building and deploying your first machine learning app in Python using Gradio",
            "Web Apps in R: Building Data-Driven Web Application in R | Shiny Tutorial Ep 3",
            "Machine Learning in R: Building a Linear Regression Model",
            "Quickly build Explainable AI dashboards in Python (explainerdashboard library)",
            "Learn Bioinformatics through Coding on ROSALIND Platform",
            "Bioinformatics Project from Scratch - Drug Discovery Part 4 (Model Building)",
            "A Summary of Deep Learning and Artificial Intelligence Landscape",
            "How to web scrape data using no code with Octoparse",
            "How to Adjust the Style of Pandas DataFrame",
            "Could this be the Best Data Science Notebook? (Deepnote)",
            "Data Pre-processing in R: Handling Missing Data",
            "Web Apps in R: How to Deploy R Shiny web app to Heroku | Shiny Tutorial Ep 6",
            "How to get started with ChatGPT for Beginners",
            "How to Build a Real-Time Transcription Web App in Python using AssemblyAI and Streamlit",
            "File Handling in Google Colab for Data Science",
            "How to Create Your Personal Data Science Learning Curriculum",
            "Wrangle and Explore Data with PANDAS-UI for Data Science",
            "How to use R and Python in same notebook on Google Colab",
            "Learn Deep Learning from NVIDIA",
            "How to build your own Speech-to-Text Transcription App in Python using AssemblyAI and Streamlit",
            "Building a Bioinformatics Web App in Python | Streamlit #7",
            "Building a No-Code Machine Learning Application (for Computer Vision) with Microsoft's Lobe",
            "Installing conda on Google Colab for Data Science",
            "Walkthrough of How I use Coding for Data Analysis",
            "How to Monitor Machine Learning Models (Evidently AI)",
            "Fool-proof RNN explanation | What are RNNs, how do they work?",
            "My thoughts and review of the Google Data Analytics Professional Certificate",
            "Data Science Virtual Internship - Part 2 (Deloitte Tech Consulting)",
            "AlphaFold 2 Paper with Code",
            "Data Science for Computational Drug Discovery using Python (Part 2 with PyCaret)",
            "How to effortlessly work with spreadsheets in Python using MITO",
            "Introduction to DagsHub for Data Science",
            "How to paraphrase text in Python using transformers",
            "How to Build Your Own AutoML App | Streamlit #17",
            "Learn Data Science for FREE with Machine Learning Mastery",
            "Becoming a Data Scientist (To PhD or not to PhD)",
            "Data Science Virtual Internship - Part 3 (GE Data Analytics)",
            "How to use Bamboolib for Data Wrangling in Data Science",
            "How to use Spreadsheet in Python with Mito",
            "How Many Hidden Layers and Neurons does a Neural Network Need",
            "DataPrep Python library for Easy Data Preparation and EDA",
            "Using Computer Code to Decipher Genetic Code - Part 1 (Bioinformatics 101)",
            "How to do research? and How to write a research paper?",
            "How to deploy machine learning model as an app in Python using Gradio",
            "How to use Grepper for Data Science",
            "How to Build a Simple Portfolio Website for FREE",
            "How to paraphrase text in Python using the PARROT library (Ft. @KenJee_ds)",
            "How to build a Cryptocurrency Price App in Python using Binance API | Streamlit #28",
            "This ONE thing helped me learn to CODE",
            "Facebook Field Guide to Machine Learning (FREE Course in Data Science)",
            "How to build a portfolio website for data science | Hugo + Hostinger",
            "How to build an Avatar maker app in 20 minutes (Python weekend project) | Streamlit #18",
            "Bioinformatics Project from Scratch - Drug Discovery Part 5 (Compare Models)",
            "Build a Machine Learning Model for Computational Drug Discovery from Scratch (Weka Tutorial #4)",
            "How to Master Python for Data Science",
            "Level Up Your Data Science Skills #shorts",
            "Making Sense of Data with Explainable AI (shapash Python library)",
            "Building a web app in Python for analyzing YouTube channels",
            "Progress Bars for Tracking the Progress of Data Science Workflow (Tqdm Python library)",
            "Web Apps in R: Building the Machine Learning Web Application in R | Shiny Tutorial Ep 4",
            "dabl - A Python library for AutoEDA and AutoML",
            "How I would learn to code (If I had to start over)",
            "Data Visualization Libraries For Python",
            "How to use GitHub Codespaces for Coding and Data Science",
            "How to Hide Password and API keys in Streamlit Share",
            "Introduction to Computational Drug Discovery",
            "Tips and Tricks for building web applications in Python with Streamlit (Ft. Avra)",
            "Find missing values in data with Pandas | Beginner tutorial",
            "Kite: Free AI Coding Assistant + Giveaway",
            "What is one-hot encoding?",
            "Deepmind's Alphafold2 Solves Protein Structures (Part 1) #shorts",
            "How to automate data processing in Python with Mito",
            "How I would learn Python",
            "Easy webscraping in Python",
            "Bioinformatics Project from Scratch - Drug Discovery #6 (Deploy Model as Web App) | Streamlit #22",
            "How to stay up-to-date in data science",
            "How to get started with Pandas for Data Science",
            "Pandas fundamentals every data scientist needs to know - Part 1",
            "How to customize the themes of your Streamlit web apps | Streamlit #24",
            "WEKA Tutorial #1.1 - How to Build a Data Mining Model from Scratch",
            "How to easily perform statistical analysis in Python with the Pengouin library",
            "Overfitting and underfitting, explained intuitively",
            "How to Build a Machine Learning Hyperparameter Optimization App | Streamlit #14",
            "A Peek into the Google Professional Certificate in Data Analytics (and IT Support)",
            "Live Coding a Streamlit App for Data Science from Scratch",
            "How to build a Bioinformatics web app (Molecular Descriptor Calculator) in Python | Streamlit #21",
            "How I Use Notion as a Professor and Content Creator in Data Science (Template Included)",
            "How to build machine learning models for drug discovery using PaDELPy",
            "Tons of FREE Data for Data Science (TidyTuesday)",
            "Faster Data Analysis in Python with Mito",
            "How to build a Portfolio website that supports Jupyter notebooks using fastpages",
            "Top 5 Python Libraries for Data Visualization (Ft. @CodingProfessor)",
            "How to come up with Data Science projects - From Ideas to Implementation",
            "BCG Data Science & Advanced Analytics Virtual Experience Program",
            "Data Science Virtual Internship - Part 11 (Quantium Data Analytics Program)",
            "How to Stay Motivated Learning Data Science",
            "How to build machine learning models for imbalanced datasets",
            "Machine Learning in R: Speed up Model Building with Parallel Computing",
            "Python Data Science in a Spreadsheet",
            "My journey into data science",
            "AutoPlotter - A GUI based Exploratory Data Analysis in Python",
            "How to use Machine Learning to explain how drugs work",
            "Exploratory Data Analysis in R: Quick Dive into Data Visualization",
            "FLAML - The AutoML from Microsoft (Machine Learning Models in 3 Lines of Code)",
            "My thoughts on web frameworks in Python and R (PyWebIO vs Streamlit vs R Shiny)",
            "Deepmind's Alphafold2 Solves Protein Structures (Part 2) #shorts",
            "How to become a Full-Stack Developer with Meta x Coursera",
            "How to Create Animated Plots in R",
            "Want Access to a High-Performance Jupyter Notebook? BlazingSQL Notebooks (Powered by NVIDIA GPUs)",
            "Data science in Psychology - Gaining insights on the Big 5 Personality Traits (Ft. Minhaaj Rehman)",
            "R Programming 101: Setting up R programming environment (R, RStudio and RStudio.cloud)",
            "Data Science Virtual Internship - Part 4 (JP Morgan Chase Software Engineering)",
            "How to Make Line Charts in Streamlit Using Plotly",
            "Python 101 for Beginners",
            "How to Build a Cryptocurrency Price Web App - Streamlit Tutorial #12",
            "How to Build Classification Models for the Penguins Dataset (Weka Tutorial #3)",
            "How to create and share beautiful images of your source code with CARBON #shorts",
            "Batch Normalization | How does it work, how to implement it (with code)",
            "The Data Science Process - A Visual Guide (Part 2)",
            "Build AI Apps with H2O Wave (from H2O.ai)",
            "Learn AI for FREE (Elements of AI)",
            "How to use Mito to automatically generate Python code for data visualization",
            "Data Science Podcast with Pat Walters (Cheminformatics Scientist)",
            "What is the best way to learn data science?",
            "Ligand-based drug discovery | Online drug discovery course",
            "How Forward Propagation in Neural Networks works",
            "How to Perform Data Splitting (Weka Tutorial #5)",
            "How to use the ChEMBL database | Online drug discovery course",
            "AlphaFold 2 Learns the Entire Human Proteome (AlphaFold Protein Structure Database)",
            "How to Build a Simple Bioinformatics Web App in Python | Streamlit #8",
            "Spreadsheets in Python with Mito library",
            "Learn Python for Data Science (with Real Python)",
            "Quick explanation: One-hot encoding",
            "Web Apps in R: Build BMI Calculator web application in R for health monitoring | Shiny Tutorial Ep 5",
            "Python vs. R comparison (by a die-hard Python fan)",
            "How to use ChatGPT to Explain Code",
            "Let's Build a Pomodoro Web App for Data Science | Streamlit #15",
            "Streamlit 101 - How to setup your Streamlit working environment",
            "MLOps 101 - A Practical Tutorial on Creating a Machine Learning Project with DagsHub",
            "How are training and tuning different?",
            "How to Use Mito for Data Analysis in Python",
            "Interpretable Machine Learning Models",
            "How to Become a Data Scientist at FAANG (Ft. Tina Huang)",
            "How to build a Linktree Clone using Python + Streamlit",
            "Data Science Virtual Internship - Part 9 (NSW Government)",
            "A Review of Hyperparameter Tuning Techniques for Neural Networks",
            "How to automatically clean data with BitRook",
            "Using Computer Code to Decipher Genetic Code - Part 2 (Bioinformatics 101)",
            "How to get started with the 66 Days of Data challenge",
            "How to fix missing values in your data",
            "Free computing resource for data science",
            "Learn Data Science with Medium.com",
            "How to Make Pie Charts in Streamlit Using Plotly",
            "What to learn to become a data scientist",
            "How to Build a Machine Learning Model Performance Calculator App",
            "Welcome to the Data Professor YouTube channel",
            "How to Create an Engaging README for your Data Science Project on GitHub",
            "Data Science Virtual Internship - Part 10 (Data@ANZ Program)",
            "How to build a web app for Drug Discovery in Python | Streamlit #26",
            "How I would learn Python",
            "Backward Propagation in Neural Networks explained",
            "The Future of this Data Science YouTube Channel",
            "How to fix missing values in data - Part 2",
            "How to Download Wikipedia",
            "Key Concepts and Techniques for Natural Language Processing",
            "How to Built an App for Converting Video to Animated Images in Python (Full code inside)",
            "Probably the Best Platform to Find Online Courses in Data Science (FREE)",
            "Data Science Podcast with Tyler Richards (Facebook Data Scientist)",
            "Explainable AI in Python with LIME (Ft. Diogo Resende)",
            "Neural Networks Hyperparameters Explained | How to set up your neural network",
            "Data Science Virtual Internship - Part 12 (Data Science & AI at Y Combinator)",
            "How to build an app for combining the contents of multiple spreadsheets | Streamlit #23",
            "Exploratory Data Analysis in Python using Mito",
            "Fixing missing values in data - Part 1",
            "Machine Learning in R: Deploy Machine Learning Model using RDS",
            "How to handle missing data in R (Ft. @StatisticsGlobe)",
            "Difference between Machine Learning and Deep Learning",
            "GPU Computing for Data Science (Unboxing NVIDIA TITAN RTX)",
            "How to Use the Open-Source Hugging Chat API in Python",
            "How to build a Currency Converter App | Streamlit #25",
            "How to run R and Python together in a Streamlit app",
            "How to set up the R programming environment | R Tutorial #1",
            "How to Harness GPU to Speed Up Machine Learning with Hummingbird-ML",
            "What is Vanishing/Exploding Gradients Problem in NNs",
            "WEKA Tutorial #1.2 - How to Build a Data Mining Model from Scratch",
            "Splitting the data into training, testing and validation datasets",
            "Data Science Virtual Internship - Part 8 (Careers in Tech by Commonwealth Bank)",
            "How to use ChatGPT to Generate Code in 90 seconds",
            "Pandas fundamentals every data scientist needs to know - Part 2",
            "How to Run Data Science Projects on the Cloud with Code Ocean (Reproducible Data Science)",
            "Tips for learning to code",
            "Exploring and Analyzing Data with Mito",
            "Which Loss Function, Optimizer and LR to Choose for Neural Networks",
            "Streamlit LLM Hackathon",
            "What does it mean to have a data-driven mindset?",
            "All Hyperparameters of a Neural Network Explained",
            "NVIDIA GTC21 (The AI Conference) is FREE + Course Giveaway",
            "How to create graphs using Mito",
            "Pandas fundamentals every data scientist needs to know - Part 3",
            "How to use Llama2 locally",
            "How to handle missing data with Pandas",
            "Pandas fundamentals every data scientist needs to know - Part 4",
            "How to make a Histogram plot in Python using Matplotlib | Ft.@CodingProfessor",
            "Building a Simple Digits Image Classification Model",
            "How to build VennLit App for comparing lists in Python using Streamlit",
            "The only explanation you need for bias and variance in Data Science",
            "Penguins Dataset as Alternative to Iris Dataset for Data Science",
            "Data Science Podcast on Time Series Prediction with Ben Auffarth",
            "How to use Mito for pre-processing datasets in Python (low code approach)",
            "What if Data Science was a Building Block? (Dolphyn)",
            "Data Science 101: Basic Command-Line for Data Science",
            "How to Build Your First Neural Network in Python and Keras",
            "Streamlit 101 - How to clone and reproduce a Streamlit web app from SCRATCH",
            "How to unleash the power of ChatGPT on your Pandas Dataframes with Mito AI",
            "Data Slicing in Python with Mito",
            "Setting up the Prerequisites to Build your First Neural Network",
            "Using Mito to make Machine Learning Easier",
            "1 Year on YouTube as the Data Professor (Data Science YouTube Channel)",
            "So how is a machine learning model built?",
            "How to quickly prototype your Streamlit web app",
            "How Does Batch Normalization Work",
            "When Should You Use L1/L2 Regularization",
            "How to Get Ahead of 99% of Data Scientists with Streamlit (Tips from Tyler Richards)",
            "Importing and Preparing a Dataset for Neural Networks",
            "How NVIDIA NGC can be used for Data Science (Podcast + Giveaway)",
            "How to build a Google Scholar App | Streamlit #30",
            "How to Become a Data Analyst (Featuring @johndavidariansen)",
            "Data Science Podcast with @misraturp",
            "Choosing a Programming Language to Learn!",
            "How to Evaluate Neural Network Performance",
            "How to examine chance prediction of a machine learning model (Y-Scrambling / Y-Permutation)",
            "Normalizing data for better Neural Network performance",
            "What is Dropout Regularization | How is it different?",
            "Data Science Virtual Internship - Part 5 (Y Combinator Startup)",
            "I deepfaked my own voice",
            "Learn about Artificial Intelligence Use Cases from YouTube Originals",
            "Data Science Interview (Tips and Tricks) [Ft. Data Science Jay]",
            "How (and Why) to Use Mini-Batches in Neural Networks",
            "Deep learning fundamentals: What is regularization? Why does it work?",
            "How to Choose an Activation Function for Neural Networks",
            "Pandas for Data Science: Create and Combine DataFrames / Rename Columns",
            "The Difference Between Data Scientist, Analyst and Engineer",
            "How to Use Learning Rate Scheduling for Neural Network Training",
            "Making Scatter Plots in R [Data Visualisation in R series]",
            "Data Science Podcast with Nate at StrataScratch",
            "Podcast with Minhaaj Rehman on Data Science and Psychology",
            "Machine Learning in R: Repurpose Machine Learning Code for New Data",
            "What is deep learning? | Intuitive explanation",
            "Building a Classification Model of the Penguins Data using Visual Programming (Orange Tutorial #2)",
            "Data Science Virtual Internship - Part 6 (BCG in Digital Transformation)",
            "Pandas fundamentals every data scientist needs to know - Part 5",
            "Cloud Dataholics - Sitcom for Data Enthusiast",
            "Who is @KenJee_ds? (Data science and Sports analytics)",
            "The upcoming NVIDIA GTC is FREE + Giveaways (Swag Bag and Course Credits)",
            "WEKA Tutorial #1.3 - How to Build a Data Mining Model from Scratch",
            "How to set up the Hyperparameters of a Neural Network",
            "Why Regularization Lowers Overfitting",
            "Talking with Founders / Developers of MITO Python Library | Data Science Podcast",
            "Ask Me Anything About Bioinformatics #2",
            "Data Science in Industry (Ft. RichardOnData)",
            "R Programming 101: How to Define Variables",
            "Data Science Podcast with Jess Haberman from Anaconda",
            "How to build a content moderation app in Python for analyzing audio files",
            "Learn about the R Data Types | R Tutorial #3",
            "Pruning a neural Network for faster training times",
            "Streamlit Hackathon - Ready, Set, Build!",
            "Identifying the PROBLEM to solve when learning to CODE",
            "How to use Pandas Profiling on Kaggle",
            "Hot topics of AI in 2022 🔥",
            "R Programming 101: Read and Write CSV files",
            "How to Select the Right Activation Function and Batch Size",
            "Data Science Virtual Internship - Part 7 (McGrathNicol in Cybersecurity Data)",
            "How to create variables and lists in R | R Tutorial #4",
            "A podcast with Chanin Nantasenamat AKA @DataProfessor",
            "How to Choose the Correct Initializer for your Neural Network",
            "How to Implement Regularization on Neural Networks",
            "Advanced Methods for Hyperparameter Tuning",
            "Weight Initialization and Regularization Techniques for NNs",
            "Ask Me Anything About Bioinformatics #3",
            "Introducing the Mito Streamlit component",
            "How to be effective in your job search: lessons from my transition from corporate to start-up",
            "Regularization with Data Augmentation and Early Stopping",
            "Gradient Clipping and How it Helps with Exploding Gradients in Neural Networks",
            "How to Evaluate a Neural Network's Performance",
            "Basics of Recurrent Neural Networks",
            "How to Make Neural Networks Train Faster on Keras",
            "The AI Conference NVIDIA GTC 22 kicks off today (Giveaway, details inside)",
            "Simple Methods for Hyperparameter Tuning",
            "Streamlit at tech conference Build 22",
            "How to install R packages | R Tutorial #2",
            "How to select the correct optimizer for Neural Networks",
            "Get ahead of the competition using your work experience",
            "Basics of Convolutional Neural Networks",
            "How to Tune a Neural Network",
            "How to Implement RNNs in Keras",
            "How to Solve Vanishing Gradients in Keras and Python",
            "ChatGPT a threat for schools?",
            "A glimpse into the future of LLMs with State of LLM Apps 2023",
            "Data Science 101: Starting a Data Science / Data Mining Project",
            "LSTMs and GRUs",
            "What is Artificial Intelligence and How is It Used in Data Science?",
            "CNN follow along calculations",
            "How to Implement CNNs in Keras",
            "Answering the most asked Data Science Quora questions - Part 1",
            "How to use Jupyter Notebooks for beginners",
            "Data science tips & tricks: How to use visualizations for data exploration",
            "Introducing the Cloud Dataholics sitcom by AtScale",
            "Data Science 101: CRISP-DM - Data Mining / Data Science in 6 Steps",
            "Tuning a Neural Network | Deciding on next steps to take",
            "How to Understand What's Wrong with a Neural Network",
            "How to Lower Neural Network Training Times",
            "Quotes #1 on Big Data and Data Science",
            "On gender equality in tech and my advice for aspiring data scientists",
            "Answering the most-asked data science questions from Quora under 2 minutes - Part 2",
            "What are Jupyter Notebooks?",
            "Episode 1 - Talking about data science in consulting with Madli Kivisik",
            "Training a Network for Better Performance",
            "Part 2: Best Settings to Initialize Your NN with",
            "Episode 6 - Becoming an ML engineer and work life at Twitter with Jigyasa Grover",
            "Part 1: Getting Ready to Build your First Advanced Neural Network",
            "What is the Optimal Performance of a Neural Network?",
            "Easiest way to set up your data science environment",
            "Episode 5 - Whys and hows of a product analyst career with Kasia Rachuta",
            "How to Decide Whether Your Neural Network is Doing Well",
            "Part 5: Tips and Tricks on How to Initialize Your Neural Network",
            "Python or R: Data Professor answers!",
            "Part 3: Training the Neural Network",
            "Advice on breaking into Tech from a FAANG Developer (@CodewithVincent)",
            "Part 4: Tuning the Neural Network for Better Performance",
            "It only takes 2 minutes to master your tasklist. Productivity tips from @DataProfessor",
            "Episode 22 - Being a data science consultant at Amazon with Naz Levent",
            "Quotes #2 on Big Data and Data Science",
            "Episode 21 - Working as a data science while still studying with Khuyen Tran",
            "Quotes #5 on Big Data and Data Science",
            "Episode 20 - A look into Women in Data with Sadie St. Lawrence",
            "Episode 10 - An unexpected way of using data science and life at Adobe with Shivali Goel",
            "Quotes #3 on Big Data and Data Science",
            "Quotes #4 on Big Data and Data Science",
            "Episode 15 - Finding remote work and life of a machine learning engineer with Ceren İyim",
            "Episode 7 - Freelancing and starting your own company with Katia Stambolieva",
            "Episode 8 - Data science from two perspectives with Nikola Valešová and Petr Míchal",
            "Episode 17 - Starting a business on data with Susan Walsh, The Classification Guru",
            "Episode 18 - An unconventional path to founding an AI Start-up with Mikiko Bazeley",
            "Starting out on YouTube: why the Data Professor got into the YT business",
            "Episode 19 - Helping the government work better with AI with Nicole Janeway Bills",
            "Episode 13 - From ecology research to data science with Meg Thomason",
            "Episode 16 - Being a data scientist in a multinational tech consultancy with Rossy Nhung Nguyen",
            "Episode 3 - Being a technical writer at a big tech firm with Melissa Barr",
            "Episode 12 - Life as a PhD student working working on AI with Selene Báez Santamaría",
            "Episode 4 - A senior data scientist's life with Samantha Zeitlin",
            "Episode 2 - Discussing big bank life with Víctor García Cazorla",
            "Episode 9 - Start-up life and being a free data scientist with Yaakov Bressler",
            "Episode 14 - Economics to Data Science with Jayeeta Putatunda",
            "Episode 11 - Data Science in Research with Sakshi Mishra",
            "Data Professor Live Stream"
           ],
           [
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Mısra Turp",
            "Data Professor",
            "Mısra Turp",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Mısra Turp",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Mısra Turp",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Mısra Turp",
            "Mısra Turp",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Data Professor",
            "Mısra Turp",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Data Professor",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Data Professor",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Data Professor",
            "Mısra Turp",
            "Data Professor",
            "Mısra Turp",
            "Mısra Turp",
            "Data Professor",
            "Data Professor",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Mısra Turp",
            "Data Professor"
           ],
           [
            9426,
            4379,
            3342,
            3251,
            3247,
            2761,
            2608,
            2507,
            2467,
            2420,
            2054,
            1854,
            1852,
            1737,
            1705,
            1687,
            1630,
            1560,
            1549,
            1415,
            1337,
            1285,
            1281,
            1244,
            1203,
            1199,
            1177,
            1103,
            1094,
            1053,
            1044,
            1043,
            1024,
            1009,
            972,
            943,
            935,
            924,
            904,
            896,
            896,
            892,
            876,
            874,
            871,
            865,
            864,
            862,
            833,
            816,
            815,
            813,
            813,
            809,
            806,
            806,
            804,
            794,
            783,
            779,
            773,
            766,
            753,
            751,
            747,
            726,
            687,
            679,
            667,
            657,
            651,
            645,
            642,
            639,
            630,
            626,
            624,
            620,
            614,
            587,
            577,
            563,
            556,
            552,
            549,
            545,
            536,
            536,
            533,
            523,
            523,
            520,
            518,
            516,
            513,
            500,
            490,
            485,
            478,
            478,
            476,
            473,
            471,
            461,
            459,
            447,
            446,
            442,
            440,
            439,
            439,
            437,
            434,
            433,
            428,
            419,
            413,
            412,
            406,
            406,
            405,
            404,
            399,
            398,
            387,
            386,
            384,
            378,
            378,
            378,
            377,
            377,
            377,
            375,
            374,
            372,
            365,
            364,
            359,
            357,
            354,
            351,
            350,
            350,
            349,
            349,
            345,
            343,
            335,
            333,
            332,
            331,
            330,
            329,
            327,
            326,
            324,
            323,
            320,
            318,
            314,
            313,
            310,
            301,
            300,
            299,
            298,
            297,
            296,
            295,
            294,
            292,
            288,
            286,
            284,
            284,
            283,
            282,
            280,
            276,
            273,
            273,
            272,
            269,
            268,
            268,
            263,
            262,
            259,
            257,
            256,
            255,
            253,
            253,
            253,
            251,
            249,
            249,
            243,
            243,
            242,
            242,
            242,
            240,
            240,
            239,
            239,
            237,
            235,
            234,
            232,
            231,
            229,
            227,
            224,
            224,
            223,
            222,
            222,
            221,
            221,
            220,
            217,
            217,
            215,
            214,
            211,
            206,
            206,
            205,
            204,
            204,
            203,
            202,
            201,
            197,
            196,
            196,
            193,
            192,
            191,
            190,
            190,
            187,
            185,
            184,
            183,
            182,
            182,
            180,
            179,
            179,
            179,
            176,
            174,
            173,
            172,
            171,
            171,
            170,
            170,
            170,
            168,
            168,
            166,
            166,
            165,
            163,
            162,
            162,
            159,
            159,
            158,
            158,
            158,
            153,
            152,
            150,
            150,
            150,
            149,
            144,
            143,
            142,
            142,
            141,
            139,
            137,
            137,
            134,
            133,
            132,
            129,
            127,
            125,
            124,
            124,
            121,
            121,
            116,
            115,
            114,
            109,
            109,
            108,
            107,
            107,
            106,
            106,
            105,
            104,
            104,
            104,
            103,
            102,
            102,
            102,
            101,
            100,
            100,
            99,
            99,
            97,
            97,
            97,
            95,
            95,
            94,
            93,
            92,
            92,
            92,
            91,
            91,
            90,
            89,
            89,
            88,
            87,
            87,
            86,
            86,
            86,
            84,
            84,
            84,
            83,
            83,
            79,
            78,
            78,
            78,
            77,
            76,
            76,
            76,
            75,
            75,
            73,
            73,
            72,
            72,
            70,
            70,
            68,
            68,
            67,
            67,
            66,
            63,
            63,
            63,
            62,
            62,
            61,
            61,
            60,
            60,
            60,
            59,
            58,
            58,
            58,
            57,
            56,
            53,
            53,
            53,
            50,
            50,
            48,
            47,
            47,
            47,
            45,
            45,
            44,
            44,
            43,
            43,
            43,
            43,
            42,
            42,
            41,
            41,
            40,
            40,
            39,
            39,
            39,
            38,
            38,
            37,
            35,
            34,
            34,
            33,
            32,
            32,
            31,
            30,
            28,
            27,
            27,
            27,
            26,
            26,
            24,
            22,
            22,
            22,
            22,
            21,
            20,
            19,
            19,
            17,
            17,
            17,
            17,
            17,
            13,
            12,
            11,
            11,
            11,
            11,
            10,
            10,
            9,
            8,
            8,
            7,
            7,
            6,
            6,
            5,
            4,
            4,
            4,
            3,
            2,
            2,
            2,
            2,
            1,
            1,
            0
           ]
          ]
         },
         "header": {
          "align": "left",
          "fill": {
           "color": "paleturquoise"
          },
          "values": [
           "Video Title",
           "Channel Name",
           "Like Count"
          ]
         },
         "type": "table"
        }
       ],
       "layout": {
        "template": {
         "data": {
          "candlestick": [
           {
            "decreasing": {
             "line": {
              "color": "#000033"
             }
            },
            "increasing": {
             "line": {
              "color": "#000032"
             }
            },
            "type": "candlestick"
           }
          ],
          "contour": [
           {
            "colorscale": [
             [
              0,
              "#000011"
             ],
             [
              0.1111111111111111,
              "#000012"
             ],
             [
              0.2222222222222222,
              "#000013"
             ],
             [
              0.3333333333333333,
              "#000014"
             ],
             [
              0.4444444444444444,
              "#000015"
             ],
             [
              0.5555555555555556,
              "#000016"
             ],
             [
              0.6666666666666666,
              "#000017"
             ],
             [
              0.7777777777777778,
              "#000018"
             ],
             [
              0.8888888888888888,
              "#000019"
             ],
             [
              1,
              "#000020"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorscale": [
             [
              0,
              "#000011"
             ],
             [
              0.1111111111111111,
              "#000012"
             ],
             [
              0.2222222222222222,
              "#000013"
             ],
             [
              0.3333333333333333,
              "#000014"
             ],
             [
              0.4444444444444444,
              "#000015"
             ],
             [
              0.5555555555555556,
              "#000016"
             ],
             [
              0.6666666666666666,
              "#000017"
             ],
             [
              0.7777777777777778,
              "#000018"
             ],
             [
              0.8888888888888888,
              "#000019"
             ],
             [
              1,
              "#000020"
             ]
            ],
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorscale": [
             [
              0,
              "#000011"
             ],
             [
              0.1111111111111111,
              "#000012"
             ],
             [
              0.2222222222222222,
              "#000013"
             ],
             [
              0.3333333333333333,
              "#000014"
             ],
             [
              0.4444444444444444,
              "#000015"
             ],
             [
              0.5555555555555556,
              "#000016"
             ],
             [
              0.6666666666666666,
              "#000017"
             ],
             [
              0.7777777777777778,
              "#000018"
             ],
             [
              0.8888888888888888,
              "#000019"
             ],
             [
              1,
              "#000020"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram2d": [
           {
            "colorscale": [
             [
              0,
              "#000011"
             ],
             [
              0.1111111111111111,
              "#000012"
             ],
             [
              0.2222222222222222,
              "#000013"
             ],
             [
              0.3333333333333333,
              "#000014"
             ],
             [
              0.4444444444444444,
              "#000015"
             ],
             [
              0.5555555555555556,
              "#000016"
             ],
             [
              0.6666666666666666,
              "#000017"
             ],
             [
              0.7777777777777778,
              "#000018"
             ],
             [
              0.8888888888888888,
              "#000019"
             ],
             [
              1,
              "#000020"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "icicle": [
           {
            "textfont": {
             "color": "white"
            },
            "type": "icicle"
           }
          ],
          "sankey": [
           {
            "textfont": {
             "color": "#000036"
            },
            "type": "sankey"
           }
          ],
          "scatter": [
           {
            "marker": {
             "line": {
              "width": 0
             }
            },
            "type": "scatter"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#000038"
             },
             "font": {
              "color": "#000037"
             },
             "line": {
              "color": "#000039"
             }
            },
            "header": {
             "fill": {
              "color": "#000040"
             },
             "font": {
              "color": "#000036"
             },
             "line": {
              "color": "#000039"
             }
            },
            "type": "table"
           }
          ],
          "waterfall": [
           {
            "connector": {
             "line": {
              "color": "#000036",
              "width": 2
             }
            },
            "decreasing": {
             "marker": {
              "color": "#000033"
             }
            },
            "increasing": {
             "marker": {
              "color": "#000032"
             }
            },
            "totals": {
             "marker": {
              "color": "#000034"
             }
            },
            "type": "waterfall"
           }
          ]
         },
         "layout": {
          "coloraxis": {
           "colorscale": [
            [
             0,
             "#000011"
            ],
            [
             0.1111111111111111,
             "#000012"
            ],
            [
             0.2222222222222222,
             "#000013"
            ],
            [
             0.3333333333333333,
             "#000014"
            ],
            [
             0.4444444444444444,
             "#000015"
            ],
            [
             0.5555555555555556,
             "#000016"
            ],
            [
             0.6666666666666666,
             "#000017"
            ],
            [
             0.7777777777777778,
             "#000018"
            ],
            [
             0.8888888888888888,
             "#000019"
            ],
            [
             1,
             "#000020"
            ]
           ]
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#000021"
            ],
            [
             0.1,
             "#000022"
            ],
            [
             0.2,
             "#000023"
            ],
            [
             0.3,
             "#000024"
            ],
            [
             0.4,
             "#000025"
            ],
            [
             0.5,
             "#000026"
            ],
            [
             0.6,
             "#000027"
            ],
            [
             0.7,
             "#000028"
            ],
            [
             0.8,
             "#000029"
            ],
            [
             0.9,
             "#000030"
            ],
            [
             1,
             "#000031"
            ]
           ],
           "sequential": [
            [
             0,
             "#000011"
            ],
            [
             0.1111111111111111,
             "#000012"
            ],
            [
             0.2222222222222222,
             "#000013"
            ],
            [
             0.3333333333333333,
             "#000014"
            ],
            [
             0.4444444444444444,
             "#000015"
            ],
            [
             0.5555555555555556,
             "#000016"
            ],
            [
             0.6666666666666666,
             "#000017"
            ],
            [
             0.7777777777777778,
             "#000018"
            ],
            [
             0.8888888888888888,
             "#000019"
            ],
            [
             1,
             "#000020"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#000011"
            ],
            [
             0.1111111111111111,
             "#000012"
            ],
            [
             0.2222222222222222,
             "#000013"
            ],
            [
             0.3333333333333333,
             "#000014"
            ],
            [
             0.4444444444444444,
             "#000015"
            ],
            [
             0.5555555555555556,
             "#000016"
            ],
            [
             0.6666666666666666,
             "#000017"
            ],
            [
             0.7777777777777778,
             "#000018"
            ],
            [
             0.8888888888888888,
             "#000019"
            ],
            [
             1,
             "#000020"
            ]
           ]
          },
          "colorway": [
           "#000001",
           "#000002",
           "#000003",
           "#000004",
           "#000005",
           "#000006",
           "#000007",
           "#000008",
           "#000009",
           "#000010"
          ]
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"02f7e5c0-ffd5-4ba0-a967-859225247f02\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"02f7e5c0-ffd5-4ba0-a967-859225247f02\")) {                    Plotly.newPlot(                        \"02f7e5c0-ffd5-4ba0-a967-859225247f02\",                        [{\"cells\":{\"align\":\"left\",\"fill\":{\"color\":\"lavender\"},\"values\":[[\"How to Build Your First Data Science Web App in Python - Streamlit Tutorial #1\",\"Build your first machine learning model in Python\",\"Bioinformatics Project from Scratch - Drug Discovery Part 1 (Data Collection and Pre-Processing)\",\"Web Apps in R: Building your First Web Application in R | Shiny Tutorial Ep 1\",\"The Art of Learning Data Science (How to learn data science)\",\"How to Do Data Cleaning (step-by-step tutorial on real-life dataset)\",\"How to Do Data Exploration (step-by-step tutorial on real-life dataset)\",\"How to Build a Simple Machine Learning Web App in Python - Streamlit Tutorial #2\",\"Streamlit Elements You Should Know About in 2023\",\"How to Build a Streamlit App (Beginner level Streamlit tutorial) - Part 1\",\"Machine Learning in Python: Building a Linear Regression Model\",\"Machine Learning for Drug Discovery (Explained in 2 minutes)\",\"How to Build Regression Models (Weka Tutorial #1)\",\"An Introduction to Computational Drug Discovery\",\"Easy Web Scraping in Python using Pandas for Data Science\",\"Building your Data Science Portfolio with GitHub (Data Science 101)\",\"Exploratory Data Analysis in Python using pandas\",\"Machine Learning in Python: Building a Classification Model\",\"Amazon's Machine Learning University (FREE Courses in Data Science)\",\"How to Implement Decision Trees in Python (Train, Test, Evaluate, Explain)\",\"Data Science 101: Deploying your Machine Learning Model\",\"Building a Dashboard web app in Python - Full Streamlit Tutorial\",\"FREE Online Courses in Data Science from Harvard University\",\"How a Biologist became a Data Scientist\",\"Strategies for Learning Data Science in 2020 (Data Science 101)\",\"How to Collect User Input with Streamlit - Part 2\",\"Data Science Virtual Internship - Part 1 (KPMG Data Analytics Consulting)\",\"How to Deploy Data Science Web App to Heroku | Streamlit #4\",\"How to Build Bioinformatics Tools\",\"How to Build a Machine Learning Model\",\"Exploratory Data Analysis in R: Towards Data Understanding\",\"My 3 go-to places to find interesting real-life datasets\",\"How to handle imbalanced datasets in Python\",\"How to Build a Penguin Classification Web App in\\u00a0Python | Streamlit #3\",\"Data Science for Computational Drug Discovery using Python (Part 1)\",\"How to Deploy a Streamlit App - Part 4\",\"Pandas functions: merge vs. join vs. concat\",\"How to Deploy Data Science Web App to Streamlit Sharing - Streamlit Tutorial #11\",\"How to write a great looking research article using LaTeX on Overleaf\",\"How to use the Pandas GroupBy function | Pandas tutorial\",\"Hyperparameter Tuning of Machine Learning Model in Python\",\"How to build your first simple web application in Python with PyWebIO\",\"Call for Participation in the Open Bioinformatics Research Project\",\"How to use Streamlit session states and callback functions | Make your apps remember things!\",\"Learn Pandas in 20 minutes!\",\"Pandas Functions: Apply vs. Map vs. Applymap\",\"Pandas Profiling for Data Science (Quick and Easy Exploratory Data Analysis)\",\"Practical Introduction to Google Colab for Data Science\",\"How to Quickly Perform Exploratory Data Analysis (EDA) in Python using Sweetviz\",\"How to stack machine learning models in Python\",\"How to use PandasGUI for Exploratory Data Analysis and Data Science\",\"How to Upload Files to Google Colab\",\"Ep. 1 How to create your first Streamlit web app in 1 minute #shorts\",\"How to Animate Plots on Streamlit, Bring your Plots to Life!\",\"Bioinformatics Project from Scratch - Drug Discovery Part 2 (Exploratory Data Analysis)\",\"Computational Drug Discovery: Machine Learning for Making Sense of Big Data in Drug Discovery\",\"How to Plot an ROC Curve in Python | Machine Learning in Python\",\"How to Save and Download files from Google Colab\",\"FREE Machine Learning Crash Course from Google\",\"Machine Learning in Python: Principal Component Analysis (PCA) for Handling High-Dimensional Data\",\"How to summarize text using ChatGPT\",\"Quick tour of PyCaret (a low-code machine learning library in Python)\",\"How to Make a Multi-Page Web App | Streamlit #16\",\"How to use the Llama 2 LLM in Python\",\"Data Science 101: Overview of Machine Learning Model Building Process\",\"How to Build a Dashboard Web App in Python with Streamlit\",\"How to build a machine learning model in Python from scratch\",\"Machine Learning in R: Building a Classification Model\",\"Save time with Pandas Grouper!\",\"How to Integrate Machine Learning to Streamlit - Part 3\",\"How to Replace Values of Dataframes | Replace, Where, Mask, Update and More\",\"How to Reshape Dataframes | Pivot, Stack, Melt and More\",\"How to Build Classification Models (Weka Tutorial #2)\",\"How to apply custom CSS styles in Streamlit apps\",\"How to build an Exploratory Data Analysis app using Pandas Profiling | Streamlit #19\",\"Ask Me Anything About Bioinformatics #1\",\"Pandas Functions: Three Ways to Use the Apply Function\",\"How to build a data science resume (portfolio website) in Python with Streamlit\",\"How to Build a Machine Learning App | Streamlit #13\",\"Web Apps in R: Build Interactive Histogram Web Application in R | Shiny Tutorial Ep 2\",\"The Data Science Process - A Visual Guide (Part 1)\",\"How to Make Tables in Streamlit Using Plotly\",\"How to Build a Football Data Web App in Python | Streamlit #9\",\"Data Science Portfolio Tips | Discussion with Ken Jee, Krish Naik, Codebasics and Data Professor\",\"How to Become a Data Scientist (Learning Path and Skill Sets Needed)\",\"Using Python in R\",\"Data Science for Bioinformatics\",\"Use native R on Google Colab for Data Science\",\"How to build a machine learning model to predict antimicrobial peptides (End-to-end Bioinformatics)\",\"Why do we split data into train test and validation sets?\",\"Learn Data Science for Free with Kaggle Micro-Courses\",\"Is it Possible to Add a Navigation Bar to Streamlit Apps? | Streamlit #29\",\"Bioinformatics Project from Scratch - Drug Discovery Part 3 (Dataset Preparation)\",\"Fastest way to upload files from Google Drive to Google Colab\",\"What programming language to learn for Data Science? R versus Python\",\"How to quickly explore data in Python using the D-Tale library\",\"Vaex - Fast data frame for Data Science (Handle billion rows in seconds)\",\"Compare Machine Learning Classifiers in Python\",\"Ep. 2 How to create a finance web app in 1 minute using Streamlit #shorts\",\"How to create your first data science project\",\"Papers With Code (Free Resource of Machine Learning Papers and Code)\",\"How to compare machine learning classifiers in 2 lines of code (lazypredict Python library)\",\"How to build a protein structure prediction app in Python using ESMFold and Streamlit\",\"Build a data storytelling web app in Python with ipyvizzu | Streamlit tutorial\",\"Build machine learning models in Google Sheets\",\"How to Build your First Classification Model using Visual Programming (Orange Tutorial #1)\",\"How to build a Stock price web app in Python | Streamlit #20\",\"How to Build a Stocks Price Web App in Python | Streamlit #10\",\"How to Build a Basketball Player Data Explorer Web App in Python | Streamlit #5\",\"This Book will Help you Land a Data Science Job\",\"How Do Decision Trees Work (Simple Explanation) - Learning and Training Process\",\"How to Speed Up Data Cleaning and Exploratory Data Analysis in Python with klib\",\"How to Build a Boston Housing Price Prediction Web App in Python | Streamlit #6\",\"How to Install and Use Pandas Profiling on Google Colab\",\"Building and deploying your first machine learning app in Python using Gradio\",\"Web Apps in R: Building Data-Driven Web Application in R | Shiny Tutorial Ep 3\",\"Machine Learning in R: Building a Linear Regression Model\",\"Quickly build Explainable AI dashboards in Python (explainerdashboard library)\",\"Learn Bioinformatics through Coding on ROSALIND Platform\",\"Bioinformatics Project from Scratch - Drug Discovery Part 4 (Model Building)\",\"A Summary of Deep Learning and Artificial Intelligence Landscape\",\"How to web scrape data using no code with Octoparse\",\"How to Adjust the Style of Pandas DataFrame\",\"Could this be the Best Data Science Notebook? (Deepnote)\",\"Data Pre-processing in R: Handling Missing Data\",\"Web Apps in R: How to Deploy R Shiny web app to Heroku | Shiny Tutorial Ep 6\",\"How to get started with ChatGPT for Beginners\",\"How to Build a Real-Time Transcription Web App in Python using AssemblyAI and Streamlit\",\"File Handling in Google Colab for Data Science\",\"How to Create Your Personal Data Science Learning Curriculum\",\"Wrangle and Explore Data with PANDAS-UI for Data Science\",\"How to use R and Python in same notebook on Google Colab\",\"Learn Deep Learning from NVIDIA\",\"How to build your own Speech-to-Text Transcription App in Python using AssemblyAI and Streamlit\",\"Building a Bioinformatics Web App in Python | Streamlit #7\",\"Building a No-Code Machine Learning Application (for Computer Vision) with Microsoft's Lobe\",\"Installing conda on Google Colab for Data Science\",\"Walkthrough of How I use Coding for Data Analysis\",\"How to Monitor Machine Learning Models (Evidently AI)\",\"Fool-proof RNN explanation | What are RNNs, how do they work?\",\"My thoughts and review of the Google Data Analytics Professional Certificate\",\"Data Science Virtual Internship - Part 2 (Deloitte Tech Consulting)\",\"AlphaFold 2 Paper with Code\",\"Data Science for Computational Drug Discovery using Python (Part 2 with PyCaret)\",\"How to effortlessly work with spreadsheets in Python using MITO\",\"Introduction to DagsHub for Data Science\",\"How to paraphrase text in Python using transformers\",\"How to Build Your Own AutoML App | Streamlit #17\",\"Learn Data Science for FREE with Machine Learning Mastery\",\"Becoming a Data Scientist (To PhD or not to PhD)\",\"Data Science Virtual Internship - Part 3 (GE Data Analytics)\",\"How to use Bamboolib for Data Wrangling in Data Science\",\"How to use Spreadsheet in Python with Mito\",\"How Many Hidden Layers and Neurons does a Neural Network Need\",\"DataPrep Python library for Easy Data Preparation and EDA\",\"Using Computer Code to Decipher Genetic Code - Part 1 (Bioinformatics 101)\",\"How to do research? and How to write a research paper?\",\"How to deploy machine learning model as an app in Python using Gradio\",\"How to use Grepper for Data Science\",\"How to Build a Simple Portfolio Website for FREE\",\"How to paraphrase text in Python using the PARROT library (Ft. @KenJee_ds)\",\"How to build a Cryptocurrency Price App in Python using Binance API | Streamlit #28\",\"This ONE thing helped me learn to CODE\",\"Facebook Field Guide to Machine Learning (FREE Course in Data Science)\",\"How to build a portfolio website for data science | Hugo + Hostinger\",\"How to build an Avatar maker app in 20 minutes (Python weekend project) | Streamlit #18\",\"Bioinformatics Project from Scratch - Drug Discovery Part 5 (Compare Models)\",\"Build a Machine Learning Model for Computational Drug Discovery from Scratch (Weka Tutorial #4)\",\"How to Master Python for Data Science\",\"Level Up Your Data Science Skills #shorts\",\"Making Sense of Data with Explainable AI (shapash Python library)\",\"Building a web app in Python for analyzing YouTube channels\",\"Progress Bars for Tracking the Progress of Data Science Workflow (Tqdm Python library)\",\"Web Apps in R: Building the Machine Learning Web Application in R | Shiny Tutorial Ep 4\",\"dabl - A Python library for AutoEDA and AutoML\",\"How I would learn to code (If I had to start over)\",\"Data Visualization Libraries For Python\",\"How to use GitHub Codespaces for Coding and Data Science\",\"How to Hide Password and API keys in Streamlit Share\",\"Introduction to Computational Drug Discovery\",\"Tips and Tricks for building web applications in Python with Streamlit (Ft. Avra)\",\"Find missing values in data with Pandas | Beginner tutorial\",\"Kite: Free AI Coding Assistant + Giveaway\",\"What is one-hot encoding?\",\"Deepmind's Alphafold2 Solves Protein Structures (Part 1) #shorts\",\"How to automate data processing in Python with Mito\",\"How I would learn Python\",\"Easy webscraping in Python\",\"Bioinformatics Project from Scratch - Drug Discovery #6 (Deploy Model as Web App) | Streamlit #22\",\"How to stay up-to-date in data science\",\"How to get started with Pandas for Data Science\",\"Pandas fundamentals every data scientist needs to know - Part 1\",\"How to customize the themes of your Streamlit web apps | Streamlit #24\",\"WEKA Tutorial #1.1 - How to Build a Data Mining Model from Scratch\",\"How to easily perform statistical analysis in Python with the Pengouin library\",\"Overfitting and underfitting, explained intuitively\",\"How to Build a Machine Learning Hyperparameter Optimization App | Streamlit #14\",\"A Peek into the Google Professional Certificate in Data Analytics (and IT Support)\",\"Live Coding a Streamlit App for Data Science from Scratch\",\"How to build a Bioinformatics web app (Molecular Descriptor Calculator) in Python | Streamlit #21\",\"How I Use Notion as a Professor and Content Creator in Data Science (Template Included)\",\"How to build machine learning models for drug discovery using PaDELPy\",\"Tons of FREE Data for Data Science (TidyTuesday)\",\"Faster Data Analysis in Python with Mito\",\"How to build a Portfolio website that supports Jupyter notebooks using fastpages\",\"Top 5 Python Libraries for Data Visualization (Ft. @CodingProfessor)\",\"How to come up with Data Science projects - From Ideas to Implementation\",\"BCG Data Science & Advanced Analytics Virtual Experience Program\",\"Data Science Virtual Internship - Part 11 (Quantium Data Analytics Program)\",\"How to Stay Motivated Learning Data Science\",\"How to build machine learning models for imbalanced datasets\",\"Machine Learning in R: Speed up Model Building with Parallel Computing\",\"Python Data Science in a Spreadsheet\",\"My journey into data science\",\"AutoPlotter - A GUI based Exploratory Data Analysis in Python\",\"How to use Machine Learning to explain how drugs work\",\"Exploratory Data Analysis in R: Quick Dive into Data Visualization\",\"FLAML - The AutoML from Microsoft (Machine Learning Models in 3 Lines of Code)\",\"My thoughts on web frameworks in Python and R (PyWebIO vs Streamlit vs R Shiny)\",\"Deepmind's Alphafold2 Solves Protein Structures (Part 2) #shorts\",\"How to become a Full-Stack Developer with Meta x Coursera\",\"How to Create Animated Plots in R\",\"Want Access to a High-Performance Jupyter Notebook? BlazingSQL Notebooks (Powered by NVIDIA GPUs)\",\"Data science in Psychology - Gaining insights on the Big 5 Personality Traits (Ft. Minhaaj Rehman)\",\"R Programming 101: Setting up R programming environment (R, RStudio and RStudio.cloud)\",\"Data Science Virtual Internship - Part 4 (JP Morgan Chase Software Engineering)\",\"How to Make Line Charts in Streamlit Using Plotly\",\"Python 101 for Beginners\",\"How to Build a Cryptocurrency Price Web App - Streamlit Tutorial #12\",\"How to Build Classification Models for the Penguins Dataset (Weka Tutorial #3)\",\"How to create and share beautiful images of your source code with CARBON #shorts\",\"Batch Normalization | How does it work, how to implement it (with code)\",\"The Data Science Process - A Visual Guide (Part 2)\",\"Build AI Apps with H2O Wave (from H2O.ai)\",\"Learn AI for FREE (Elements of AI)\",\"How to use Mito to automatically generate Python code for data visualization\",\"Data Science Podcast with Pat Walters (Cheminformatics Scientist)\",\"What is the best way to learn data science?\",\"Ligand-based drug discovery | Online drug discovery course\",\"How Forward Propagation in Neural Networks works\",\"How to Perform Data Splitting (Weka Tutorial #5)\",\"How to use the ChEMBL database | Online drug discovery course\",\"AlphaFold 2 Learns the Entire Human Proteome (AlphaFold Protein Structure Database)\",\"How to Build a Simple Bioinformatics Web App in Python | Streamlit #8\",\"Spreadsheets in Python with Mito library\",\"Learn Python for Data Science (with Real Python)\",\"Quick explanation: One-hot encoding\",\"Web Apps in R: Build BMI Calculator web application in R for health monitoring | Shiny Tutorial Ep 5\",\"Python vs. R comparison (by a die-hard Python fan)\",\"How to use ChatGPT to Explain Code\",\"Let's Build a Pomodoro Web App for Data Science | Streamlit #15\",\"Streamlit 101 - How to setup your Streamlit working environment\",\"MLOps 101 - A Practical Tutorial on Creating a Machine Learning Project with DagsHub\",\"How are training and tuning different?\",\"How to Use Mito for Data Analysis in Python\",\"Interpretable Machine Learning Models\",\"How to Become a Data Scientist at FAANG (Ft. Tina Huang)\",\"How to build a Linktree Clone using Python + Streamlit\",\"Data Science Virtual Internship - Part 9 (NSW Government)\",\"A Review of Hyperparameter Tuning Techniques for Neural Networks\",\"How to automatically clean data with BitRook\",\"Using Computer Code to Decipher Genetic Code - Part 2 (Bioinformatics 101)\",\"How to get started with the 66 Days of Data challenge\",\"How to fix missing values in your data\",\"Free computing resource for data science\",\"Learn Data Science with Medium.com\",\"How to Make Pie Charts in Streamlit Using Plotly\",\"What to learn to become a data scientist\",\"How to Build a Machine Learning Model Performance Calculator App\",\"Welcome to the Data Professor YouTube channel\",\"How to Create an Engaging README for your Data Science Project on GitHub\",\"Data Science Virtual Internship - Part 10 (Data@ANZ Program)\",\"How to build a web app for Drug Discovery in Python | Streamlit #26\",\"How I would learn Python\",\"Backward Propagation in Neural Networks explained\",\"The Future of this Data Science YouTube Channel\",\"How to fix missing values in data - Part 2\",\"How to Download Wikipedia\",\"Key Concepts and Techniques for Natural Language Processing\",\"How to Built an App for Converting Video to Animated Images in Python (Full code inside)\",\"Probably the Best Platform to Find Online Courses in Data Science (FREE)\",\"Data Science Podcast with Tyler Richards (Facebook Data Scientist)\",\"Explainable AI in Python with LIME (Ft. Diogo Resende)\",\"Neural Networks Hyperparameters Explained | How to set up your neural network\",\"Data Science Virtual Internship - Part 12 (Data Science & AI at Y Combinator)\",\"How to build an app for combining the contents of multiple spreadsheets | Streamlit #23\",\"Exploratory Data Analysis in Python using Mito\",\"Fixing missing values in data - Part 1\",\"Machine Learning in R: Deploy Machine Learning Model using RDS\",\"How to handle missing data in R (Ft. @StatisticsGlobe)\",\"Difference between Machine Learning and Deep Learning\",\"GPU Computing for Data Science (Unboxing NVIDIA TITAN RTX)\",\"How to Use the Open-Source Hugging Chat API in Python\",\"How to build a Currency Converter App | Streamlit #25\",\"How to run R and Python together in a Streamlit app\",\"How to set up the R programming environment | R Tutorial #1\",\"How to Harness GPU to Speed Up Machine Learning with Hummingbird-ML\",\"What is Vanishing/Exploding Gradients Problem in NNs\",\"WEKA Tutorial #1.2 - How to Build a Data Mining Model from Scratch\",\"Splitting the data into training, testing and validation datasets\",\"Data Science Virtual Internship - Part 8 (Careers in Tech by Commonwealth Bank)\",\"How to use ChatGPT to Generate Code in 90 seconds\",\"Pandas fundamentals every data scientist needs to know - Part 2\",\"How to Run Data Science Projects on the Cloud with Code Ocean (Reproducible Data Science)\",\"Tips for learning to code\",\"Exploring and Analyzing Data with Mito\",\"Which Loss Function, Optimizer and LR to Choose for Neural Networks\",\"Streamlit LLM Hackathon\",\"What does it mean to have a data-driven mindset?\",\"All Hyperparameters of a Neural Network Explained\",\"NVIDIA GTC21 (The AI Conference) is FREE + Course Giveaway\",\"How to create graphs using Mito\",\"Pandas fundamentals every data scientist needs to know - Part 3\",\"How to use Llama2 locally\",\"How to handle missing data with Pandas\",\"Pandas fundamentals every data scientist needs to know - Part 4\",\"How to make a Histogram plot in Python using Matplotlib | Ft.@CodingProfessor\",\"Building a Simple Digits Image Classification Model\",\"How to build VennLit App for comparing lists in Python using Streamlit\",\"The only explanation you need for bias and variance in Data Science\",\"Penguins Dataset as Alternative to Iris Dataset for Data Science\",\"Data Science Podcast on Time Series Prediction with Ben Auffarth\",\"How to use Mito for pre-processing datasets in Python (low code approach)\",\"What if Data Science was a Building Block? (Dolphyn)\",\"Data Science 101: Basic Command-Line for Data Science\",\"How to Build Your First Neural Network in Python and Keras\",\"Streamlit 101 - How to clone and reproduce a Streamlit web app from SCRATCH\",\"How to unleash the power of ChatGPT on your Pandas Dataframes with Mito AI\",\"Data Slicing in Python with Mito\",\"Setting up the Prerequisites to Build your First Neural Network\",\"Using Mito to make Machine Learning Easier\",\"1 Year on YouTube as the Data Professor (Data Science YouTube Channel)\",\"So how is a machine learning model built?\",\"How to quickly prototype your Streamlit web app\",\"How Does Batch Normalization Work\",\"When Should You Use L1/L2 Regularization\",\"How to Get Ahead of 99% of Data Scientists with Streamlit (Tips from Tyler Richards)\",\"Importing and Preparing a Dataset for Neural Networks\",\"How NVIDIA NGC can be used for Data Science (Podcast + Giveaway)\",\"How to build a Google Scholar App | Streamlit #30\",\"How to Become a Data Analyst (Featuring @johndavidariansen)\",\"Data Science Podcast with @misraturp\",\"Choosing a Programming Language to Learn!\",\"How to Evaluate Neural Network Performance\",\"How to examine chance prediction of a machine learning model (Y-Scrambling / Y-Permutation)\",\"Normalizing data for better Neural Network performance\",\"What is Dropout Regularization | How is it different?\",\"Data Science Virtual Internship - Part 5 (Y Combinator Startup)\",\"I deepfaked my own voice\",\"Learn about Artificial Intelligence Use Cases from YouTube Originals\",\"Data Science Interview (Tips and Tricks) [Ft. Data Science Jay]\",\"How (and Why) to Use Mini-Batches in Neural Networks\",\"Deep learning fundamentals: What is regularization? Why does it work?\",\"How to Choose an Activation Function for Neural Networks\",\"Pandas for Data Science: Create and Combine DataFrames / Rename Columns\",\"The Difference Between Data Scientist, Analyst and Engineer\",\"How to Use Learning Rate Scheduling for Neural Network Training\",\"Making Scatter Plots in R [Data Visualisation in R series]\",\"Data Science Podcast with Nate at StrataScratch\",\"Podcast with Minhaaj Rehman on Data Science and Psychology\",\"Machine Learning in R: Repurpose Machine Learning Code for New Data\",\"What is deep learning? | Intuitive explanation\",\"Building a Classification Model of the Penguins Data using Visual Programming (Orange Tutorial #2)\",\"Data Science Virtual Internship - Part 6 (BCG in Digital Transformation)\",\"Pandas fundamentals every data scientist needs to know - Part 5\",\"Cloud Dataholics - Sitcom for Data Enthusiast\",\"Who is @KenJee_ds? (Data science and Sports analytics)\",\"The upcoming NVIDIA GTC is FREE + Giveaways (Swag Bag and Course Credits)\",\"WEKA Tutorial #1.3 - How to Build a Data Mining Model from Scratch\",\"How to set up the Hyperparameters of a Neural Network\",\"Why Regularization Lowers Overfitting\",\"Talking with Founders / Developers of MITO Python Library | Data Science Podcast\",\"Ask Me Anything About Bioinformatics #2\",\"Data Science in Industry (Ft. RichardOnData)\",\"R Programming 101: How to Define Variables\",\"Data Science Podcast with Jess Haberman from Anaconda\",\"How to build a content moderation app in Python for analyzing audio files\",\"Learn about the R Data Types | R Tutorial #3\",\"Pruning a neural Network for faster training times\",\"Streamlit Hackathon - Ready, Set, Build!\",\"Identifying the PROBLEM to solve when learning to CODE\",\"How to use Pandas Profiling on Kaggle\",\"Hot topics of AI in 2022 \\ud83d\\udd25\",\"R Programming 101: Read and Write CSV files\",\"How to Select the Right Activation Function and Batch Size\",\"Data Science Virtual Internship - Part 7 (McGrathNicol in Cybersecurity Data)\",\"How to create variables and lists in R | R Tutorial #4\",\"A podcast with Chanin Nantasenamat AKA @DataProfessor\",\"How to Choose the Correct Initializer for your Neural Network\",\"How to Implement Regularization on Neural Networks\",\"Advanced Methods for Hyperparameter Tuning\",\"Weight Initialization and Regularization Techniques for NNs\",\"Ask Me Anything About Bioinformatics #3\",\"Introducing the Mito Streamlit component\",\"How to be effective in your job search: lessons from my transition from corporate to start-up\",\"Regularization with Data Augmentation and Early Stopping\",\"Gradient Clipping and How it Helps with Exploding Gradients in Neural Networks\",\"How to Evaluate a Neural Network's Performance\",\"Basics of Recurrent Neural Networks\",\"How to Make Neural Networks Train Faster on Keras\",\"The AI Conference NVIDIA GTC 22 kicks off today (Giveaway, details inside)\",\"Simple Methods for Hyperparameter Tuning\",\"Streamlit at tech conference Build 22\",\"How to install R packages | R Tutorial #2\",\"How to select the correct optimizer for Neural Networks\",\"Get ahead of the competition using your work experience\",\"Basics of Convolutional Neural Networks\",\"How to Tune a Neural Network\",\"How to Implement RNNs in Keras\",\"How to Solve Vanishing Gradients in Keras and Python\",\"ChatGPT a threat for schools?\",\"A glimpse into the future of LLMs with State of LLM Apps 2023\",\"Data Science 101: Starting a Data Science / Data Mining Project\",\"LSTMs and GRUs\",\"What is Artificial Intelligence and How is It Used in Data Science?\",\"CNN follow along calculations\",\"How to Implement CNNs in Keras\",\"Answering the most asked Data Science Quora questions - Part 1\",\"How to use Jupyter Notebooks for beginners\",\"Data science tips & tricks: How to use visualizations for data exploration\",\"Introducing the Cloud Dataholics sitcom by AtScale\",\"Data Science 101: CRISP-DM - Data Mining / Data Science in 6 Steps\",\"Tuning a Neural Network | Deciding on next steps to take\",\"How to Understand What's Wrong with a Neural Network\",\"How to Lower Neural Network Training Times\",\"Quotes #1 on Big Data and Data Science\",\"On gender equality in tech and my advice for aspiring data scientists\",\"Answering the most-asked data science questions from Quora under 2 minutes - Part 2\",\"What are Jupyter Notebooks?\",\"Episode 1 - Talking about data science in consulting with Madli Kivisik\",\"Training a Network for Better Performance\",\"Part 2: Best Settings to Initialize Your NN with\",\"Episode 6 - Becoming an ML engineer and work life at Twitter with Jigyasa Grover\",\"Part 1: Getting Ready to Build your First Advanced Neural Network\",\"What is the Optimal Performance of a Neural Network?\",\"Easiest way to set up your data science environment\",\"Episode 5 - Whys and hows of a product analyst career with Kasia Rachuta\",\"How to Decide Whether Your Neural Network is Doing Well\",\"Part 5: Tips and Tricks on How to Initialize Your Neural Network\",\"Python or R: Data Professor answers!\",\"Part 3: Training the Neural Network\",\"Advice on breaking into Tech from a FAANG Developer (@CodewithVincent)\",\"Part 4: Tuning the Neural Network for Better Performance\",\"It only takes 2 minutes to master your tasklist. Productivity tips from @DataProfessor\",\"Episode 22 - Being a data science consultant at Amazon with Naz Levent\",\"Quotes #2 on Big Data and Data Science\",\"Episode 21 - Working as a data science while still studying with Khuyen Tran\",\"Quotes #5 on Big Data and Data Science\",\"Episode 20 - A look into Women in Data with Sadie St. Lawrence\",\"Episode 10 - An unexpected way of using data science and life at Adobe with Shivali Goel\",\"Quotes #3 on Big Data and Data Science\",\"Quotes #4 on Big Data and Data Science\",\"Episode 15 - Finding remote work and life of a machine learning engineer with Ceren \\u0130yim\",\"Episode 7 - Freelancing and starting your own company with Katia Stambolieva\",\"Episode 8 - Data science from two perspectives with Nikola Vale\\u0161ov\\u00e1 and Petr M\\u00edchal\",\"Episode 17 - Starting a business on data with Susan Walsh, The Classification Guru\",\"Episode 18 - An unconventional path to founding an AI Start-up with Mikiko Bazeley\",\"Starting out on YouTube: why the Data Professor got into the YT business\",\"Episode 19 - Helping the government work better with AI with Nicole Janeway Bills\",\"Episode 13 - From ecology research to data science with Meg Thomason\",\"Episode 16 - Being a data scientist in a multinational tech consultancy with Rossy Nhung Nguyen\",\"Episode 3 - Being a technical writer at a big tech firm with Melissa Barr\",\"Episode 12 - Life as a PhD student working working on AI with Selene B\\u00e1ez Santamar\\u00eda\",\"Episode 4 - A senior data scientist's life with Samantha Zeitlin\",\"Episode 2 - Discussing big bank life with V\\u00edctor Garc\\u00eda Cazorla\",\"Episode 9 - Start-up life and being a free data scientist with Yaakov Bressler\",\"Episode 14 - Economics to Data Science with Jayeeta Putatunda\",\"Episode 11 - Data Science in Research with Sakshi Mishra\",\"Data Professor Live Stream\"],[\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"Data Professor\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"Data Professor\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"Data Professor\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"Data Professor\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"Data Professor\",\"M\\u0131sra Turp\",\"Data Professor\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"Data Professor\",\"Data Professor\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"M\\u0131sra Turp\",\"Data Professor\"],[9426,4379,3342,3251,3247,2761,2608,2507,2467,2420,2054,1854,1852,1737,1705,1687,1630,1560,1549,1415,1337,1285,1281,1244,1203,1199,1177,1103,1094,1053,1044,1043,1024,1009,972,943,935,924,904,896,896,892,876,874,871,865,864,862,833,816,815,813,813,809,806,806,804,794,783,779,773,766,753,751,747,726,687,679,667,657,651,645,642,639,630,626,624,620,614,587,577,563,556,552,549,545,536,536,533,523,523,520,518,516,513,500,490,485,478,478,476,473,471,461,459,447,446,442,440,439,439,437,434,433,428,419,413,412,406,406,405,404,399,398,387,386,384,378,378,378,377,377,377,375,374,372,365,364,359,357,354,351,350,350,349,349,345,343,335,333,332,331,330,329,327,326,324,323,320,318,314,313,310,301,300,299,298,297,296,295,294,292,288,286,284,284,283,282,280,276,273,273,272,269,268,268,263,262,259,257,256,255,253,253,253,251,249,249,243,243,242,242,242,240,240,239,239,237,235,234,232,231,229,227,224,224,223,222,222,221,221,220,217,217,215,214,211,206,206,205,204,204,203,202,201,197,196,196,193,192,191,190,190,187,185,184,183,182,182,180,179,179,179,176,174,173,172,171,171,170,170,170,168,168,166,166,165,163,162,162,159,159,158,158,158,153,152,150,150,150,149,144,143,142,142,141,139,137,137,134,133,132,129,127,125,124,124,121,121,116,115,114,109,109,108,107,107,106,106,105,104,104,104,103,102,102,102,101,100,100,99,99,97,97,97,95,95,94,93,92,92,92,91,91,90,89,89,88,87,87,86,86,86,84,84,84,83,83,79,78,78,78,77,76,76,76,75,75,73,73,72,72,70,70,68,68,67,67,66,63,63,63,62,62,61,61,60,60,60,59,58,58,58,57,56,53,53,53,50,50,48,47,47,47,45,45,44,44,43,43,43,43,42,42,41,41,40,40,39,39,39,38,38,37,35,34,34,33,32,32,31,30,28,27,27,27,26,26,24,22,22,22,22,21,20,19,19,17,17,17,17,17,13,12,11,11,11,11,10,10,9,8,8,7,7,6,6,5,4,4,4,3,2,2,2,2,1,1,0]]},\"header\":{\"align\":\"left\",\"fill\":{\"color\":\"paleturquoise\"},\"values\":[\"Video Title\",\"Channel Name\",\"Like Count\"]},\"type\":\"table\"}],                        {\"template\":{\"data\":{\"candlestick\":[{\"decreasing\":{\"line\":{\"color\":\"#000033\"}},\"increasing\":{\"line\":{\"color\":\"#000032\"}},\"type\":\"candlestick\"}],\"contourcarpet\":[{\"colorscale\":[[0.0,\"#000011\"],[0.1111111111111111,\"#000012\"],[0.2222222222222222,\"#000013\"],[0.3333333333333333,\"#000014\"],[0.4444444444444444,\"#000015\"],[0.5555555555555556,\"#000016\"],[0.6666666666666666,\"#000017\"],[0.7777777777777778,\"#000018\"],[0.8888888888888888,\"#000019\"],[1.0,\"#000020\"]],\"type\":\"contourcarpet\"}],\"contour\":[{\"colorscale\":[[0.0,\"#000011\"],[0.1111111111111111,\"#000012\"],[0.2222222222222222,\"#000013\"],[0.3333333333333333,\"#000014\"],[0.4444444444444444,\"#000015\"],[0.5555555555555556,\"#000016\"],[0.6666666666666666,\"#000017\"],[0.7777777777777778,\"#000018\"],[0.8888888888888888,\"#000019\"],[1.0,\"#000020\"]],\"type\":\"contour\"}],\"heatmap\":[{\"colorscale\":[[0.0,\"#000011\"],[0.1111111111111111,\"#000012\"],[0.2222222222222222,\"#000013\"],[0.3333333333333333,\"#000014\"],[0.4444444444444444,\"#000015\"],[0.5555555555555556,\"#000016\"],[0.6666666666666666,\"#000017\"],[0.7777777777777778,\"#000018\"],[0.8888888888888888,\"#000019\"],[1.0,\"#000020\"]],\"type\":\"heatmap\"}],\"histogram2d\":[{\"colorscale\":[[0.0,\"#000011\"],[0.1111111111111111,\"#000012\"],[0.2222222222222222,\"#000013\"],[0.3333333333333333,\"#000014\"],[0.4444444444444444,\"#000015\"],[0.5555555555555556,\"#000016\"],[0.6666666666666666,\"#000017\"],[0.7777777777777778,\"#000018\"],[0.8888888888888888,\"#000019\"],[1.0,\"#000020\"]],\"type\":\"histogram2d\"}],\"icicle\":[{\"textfont\":{\"color\":\"white\"},\"type\":\"icicle\"}],\"sankey\":[{\"textfont\":{\"color\":\"#000036\"},\"type\":\"sankey\"}],\"scatter\":[{\"marker\":{\"line\":{\"width\":0}},\"type\":\"scatter\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#000038\"},\"font\":{\"color\":\"#000037\"},\"line\":{\"color\":\"#000039\"}},\"header\":{\"fill\":{\"color\":\"#000040\"},\"font\":{\"color\":\"#000036\"},\"line\":{\"color\":\"#000039\"}},\"type\":\"table\"}],\"waterfall\":[{\"connector\":{\"line\":{\"color\":\"#000036\",\"width\":2}},\"decreasing\":{\"marker\":{\"color\":\"#000033\"}},\"increasing\":{\"marker\":{\"color\":\"#000032\"}},\"totals\":{\"marker\":{\"color\":\"#000034\"}},\"type\":\"waterfall\"}]},\"layout\":{\"coloraxis\":{\"colorscale\":[[0.0,\"#000011\"],[0.1111111111111111,\"#000012\"],[0.2222222222222222,\"#000013\"],[0.3333333333333333,\"#000014\"],[0.4444444444444444,\"#000015\"],[0.5555555555555556,\"#000016\"],[0.6666666666666666,\"#000017\"],[0.7777777777777778,\"#000018\"],[0.8888888888888888,\"#000019\"],[1.0,\"#000020\"]]},\"colorscale\":{\"diverging\":[[0.0,\"#000021\"],[0.1,\"#000022\"],[0.2,\"#000023\"],[0.3,\"#000024\"],[0.4,\"#000025\"],[0.5,\"#000026\"],[0.6,\"#000027\"],[0.7,\"#000028\"],[0.8,\"#000029\"],[0.9,\"#000030\"],[1.0,\"#000031\"]],\"sequential\":[[0.0,\"#000011\"],[0.1111111111111111,\"#000012\"],[0.2222222222222222,\"#000013\"],[0.3333333333333333,\"#000014\"],[0.4444444444444444,\"#000015\"],[0.5555555555555556,\"#000016\"],[0.6666666666666666,\"#000017\"],[0.7777777777777778,\"#000018\"],[0.8888888888888888,\"#000019\"],[1.0,\"#000020\"]],\"sequentialminus\":[[0.0,\"#000011\"],[0.1111111111111111,\"#000012\"],[0.2222222222222222,\"#000013\"],[0.3333333333333333,\"#000014\"],[0.4444444444444444,\"#000015\"],[0.5555555555555556,\"#000016\"],[0.6666666666666666,\"#000017\"],[0.7777777777777778,\"#000018\"],[0.8888888888888888,\"#000019\"],[1.0,\"#000020\"]]},\"colorway\":[\"#000001\",\"#000002\",\"#000003\",\"#000004\",\"#000005\",\"#000006\",\"#000007\",\"#000008\",\"#000009\",\"#000010\"]}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('02f7e5c0-ffd5-4ba0-a967-859225247f02');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = '''SELECT title, channel_name, likes FROM videos \n",
    "    WHERE likes IS NOT NULL ORDER BY likes DESC\n",
    "    '''\n",
    "cursor.execute(query)\n",
    "tuples = cursor.fetchall()\n",
    "df = pd.DataFrame(tuples, columns = ['Video Title', 'Channel Name', 'Like Count'])\n",
    "df\n",
    "    \n",
    "fig = go.Figure(data=[go.Table(\n",
    "    header=dict(values=list(df.columns),\n",
    "                fill_color='paleturquoise',\n",
    "                align='left'),\n",
    "    cells=dict(values=[df['Video Title'], df['Channel Name'], df['Like Count']],\n",
    "               fill_color='lavender',\n",
    "               align='left'))\n",
    "])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8912e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
